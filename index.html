<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>The Fill-Mask Association Test â€¢ FMAT</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="apple-touch-icon-60x60.png">
<script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/Roboto-0.4.9/font.css" rel="stylesheet">
<link href="deps/Lexend-0.4.9/font.css" rel="stylesheet">
<link href="deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="The Fill-Mask Association Test">
<meta name="description" content="The Fill-Mask Association Test (FMAT) &lt;doi:10.1037/pspa0000396&gt; is an integrative and probability-based method using Masked Language Models to measure conceptual associations (e.g., attitudes, biases, stereotypes, social norms, cultural values) as propositions in natural language. Supported language models include BERT &lt;doi:10.48550/arXiv.1810.04805&gt; and its variants available at Hugging Face &lt;https://huggingface.co/models?pipeline_tag=fill-mask&gt;. Methodological references and installation guidance are provided at &lt;https://psychbruce.github.io/FMAT/&gt;.">
<meta property="og:description" content="The Fill-Mask Association Test (FMAT) &lt;doi:10.1037/pspa0000396&gt; is an integrative and probability-based method using Masked Language Models to measure conceptual associations (e.g., attitudes, biases, stereotypes, social norms, cultural values) as propositions in natural language. Supported language models include BERT &lt;doi:10.48550/arXiv.1810.04805&gt; and its variants available at Hugging Face &lt;https://huggingface.co/models?pipeline_tag=fill-mask&gt;. Methodological references and installation guidance are provided at &lt;https://psychbruce.github.io/FMAT/&gt;.">
<meta property="og:image" content="https://psychbruce.github.io/FMAT/logo.png">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">FMAT</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2025.4</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/psychbruce/FMAT/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-home">
<div class="row">
  <main id="main" class="col-md-9"><div class="section level1">
<div class="page-header">
<img src="logo.png" class="logo" alt=""><h1 id="fmat-">FMAT <a class="anchor" aria-label="anchor" href="#fmat-"></a>
</h1>
</div>
<p>ğŸ˜· The Fill-Mask Association Test (æ©ç å¡«ç©ºè”ç³»æµ‹éªŒ).</p>
<p>The <em>Fill-Mask Association Test</em> (FMAT) is an integrative and probability-based method using <a href="#bert-models">BERT Models</a> to measure conceptual associations (e.g., attitudes, biases, stereotypes, social norms, cultural values) as <em>propositions</em> in natural language (<a href="https://doi.org/10.1037/pspa0000396" class="external-link">Bao, 2024, <em>JPSP</em></a>).</p>
<p>âš ï¸ <em>Please update this package to version â‰¥ 2025.4 for faster and more robust functionality.</em></p>
<p><img src="https://psychbruce.github.io/img/FMAT-Workflow.png"></p>
<!-- badges: start -->

<!-- badges: end -->
<p><img src="https://psychbruce.github.io/img/CC-BY-NC-SA.jpg" width="120px" height="42px"></p>
<div class="section level2">
<h2 id="author">Author<a class="anchor" aria-label="anchor" href="#author"></a>
</h2>
<p>Han-Wu-Shuang (Bruce) Bao åŒ…å¯’å´éœœ</p>
<p>ğŸ“¬ <a href="mailto:baohws@foxmail.com">baohws@foxmail.com</a></p>
<p>ğŸ“‹ <a href="https://psychbruce.github.io" class="external-link">psychbruce.github.io</a></p>
</div>
<div class="section level2">
<h2 id="citation">Citation<a class="anchor" aria-label="anchor" href="#citation"></a>
</h2>
<div class="section level3">
<h3 id="id_1-fmat-package-citation">(1) FMAT Package Citation<a class="anchor" aria-label="anchor" href="#id_1-fmat-package-citation"></a>
</h3>
<ul>
<li>Bao, H.-W.-S. (2023). <em>FMAT: The Fill-Mask Association Test</em>. <a href="https://CRAN.R-project.org/package=FMAT" class="external-link uri">https://CRAN.R-project.org/package=FMAT</a>
<ul>
<li>
<em>Note</em>: This is the original citation. Please refer to the information when you <code><a href="https://psychbruce.github.io/FMAT/">library(FMAT)</a></code> for the APA-7 format of the version you installed.</li>
</ul>
</li>
</ul>
</div>
<div class="section level3">
<h3 id="id_2-fmat-research-articles---methodology">(2) FMAT Research Articles - Methodology<a class="anchor" aria-label="anchor" href="#id_2-fmat-research-articles---methodology"></a>
</h3>
<ul>
<li>Bao, H.-W.-S. (2024). The Fill-Mask Association Test (FMAT): Measuring propositions in natural language. <em>Journal of Personality and Social Psychology, 127</em>(3), 537â€“561. <a href="https://doi.org/10.1037/pspa0000396" class="external-link uri">https://doi.org/10.1037/pspa0000396</a>
</li>
</ul>
</div>
<div class="section level3">
<h3 id="id_3-fmat-research-articles---application">(3) FMAT Research Articles - Application<a class="anchor" aria-label="anchor" href="#id_3-fmat-research-articles---application"></a>
</h3>
<ul>
<li>Bao, H.-W.-S., &amp; Gries, P. (2024). Intersectional raceâ€“gender stereotypes in natural language. <em>British Journal of Social Psychology, 63</em>(4), 1771â€“1786. <a href="https://doi.org/10.1111/bjso.12748" class="external-link uri">https://doi.org/10.1111/bjso.12748</a>
</li>
<li>Bao, H.-W.-S., &amp; Gries, P. (2025). Bias toward Chinese in English language use. <em>China Quarterly</em>. (accepted)</li>
</ul>
</div>
</div>
<div class="section level2">
<h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h2>
<p>The R package <code>FMAT</code> and three Python packages (<code>transformers</code>, <code>torch</code>, <code>huggingface-hub</code>) all need to be installed.</p>
<div class="section level3">
<h3 id="id_1-r-package">(1) R Package<a class="anchor" aria-label="anchor" href="#id_1-r-package"></a>
</h3>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## Method 1: Install from CRAN</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"FMAT"</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Method 2: Install from GitHub</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"devtools"</span><span class="op">)</span></span>
<span><span class="fu">devtools</span><span class="fu">::</span><span class="fu">install_github</span><span class="op">(</span><span class="st">"psychbruce/FMAT"</span>, force<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="id_2-python-environment-and-packages">(2) Python Environment and Packages<a class="anchor" aria-label="anchor" href="#id_2-python-environment-and-packages"></a>
</h3>
<p>Install <a href="https://www.anaconda.com/download/success" class="external-link">Anaconda</a> (a recommended package manager that automatically installs Python, its IDEs like Spyder, and a large list of common Python packages).</p>
<p>Specify the Anacondaâ€™s Python interpreter in RStudio.</p>
<blockquote>
<p>RStudio â†’ Tools â†’ Global/Project Options<br>
â†’ Python â†’ Select â†’ <strong>Conda Environments</strong><br>
â†’ Choose <strong>â€œâ€¦/Anaconda3/python.exeâ€</strong></p>
</blockquote>
<p>Install specific versions of Python packages â€œ<a href="https://pypi.org/project/transformers/#history" class="external-link">transformers</a>â€, â€œ<a href="https://pypi.org/project/torch/#history" class="external-link">torch</a>â€, and â€œ<a href="https://pypi.org/project/huggingface-hub/#history" class="external-link">huggingface-hub</a>â€.<br>
(RStudio Terminal / Anaconda Prompt / Windows Command)</p>
<p>For CPU users:</p>
<pre><code>pip install transformers==4.40.2 torch==2.2.1 huggingface-hub==0.20.3</code></pre>
<p>For GPU (CUDA) users:</p>
<pre><code>pip install transformers==4.40.2 huggingface-hub==0.20.3
pip install torch==2.2.1 --index-url https://download.pytorch.org/whl/cu121</code></pre>
<p>To use some models (e.g., <code>microsoft/deberta-v3-base</code>), â€œYou need to have sentencepiece installed to convert a slow tokenizer to a fast oneâ€:</p>
<pre><code>pip install sentencepiece</code></pre>
<ul>
<li>See <a href="#guidance-for-gpu-acceleration">Guidance for GPU Acceleration</a> for installation guidance if you have an NVIDIA GPU device on your PC and want to use GPU to accelerate the pipeline.</li>
<li>According to the May 2024 releases, â€œtransformersâ€ â‰¥ 4.41 depends on â€œhuggingface-hubâ€ â‰¥ 0.23. The suggested versions of â€œtransformersâ€ (4.40.2) and â€œhuggingface-hubâ€ (0.20.3) ensure the console display of progress bars when downloading BERT models while keeping these packages as new as possible.</li>
<li>Proxy users may use the â€œglobal modeâ€ (å…¨å±€æ¨¡å¼) to download models.</li>
<li>If you find the error <code>HTTPSConnectionPool(host='huggingface.co', port=443)</code>, please try to (1) reinstall <a href="https://www.anaconda.com/download/success" class="external-link">Anaconda</a> so that some unknown issues may be fixed, or (2) downgrade the â€œ<a href="https://pypi.org/project/urllib3/" class="external-link">urllib3</a>â€ package to version â‰¤ 1.25.11 (<code>pip install urllib3==1.25.11</code>) so that it will use HTTP proxies (rather than HTTPS proxies as in later versions) to connect to Hugging Face.
<ul>
<li><a href="https://www.cnblogs.com/xyz/p/17872452.html" class="external-link uri">https://www.cnblogs.com/xyz/p/17872452.html</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section level2">
<h2 id="guidance-for-fmat">Guidance for FMAT<a class="anchor" aria-label="anchor" href="#guidance-for-fmat"></a>
</h2>
<div class="section level3">
<h3 id="step-1-download-bert-models">Step 1: Download BERT Models<a class="anchor" aria-label="anchor" href="#step-1-download-bert-models"></a>
</h3>
<p>Use <code><a href="reference/BERT_download.html">BERT_download()</a></code> to download <a href="#bert-models">BERT models</a>. Model files are saved in your local cache folder â€œ%USERPROFILE%/.cache/huggingfaceâ€. A full list of BERT models are available at <a href="https://huggingface.co/models?pipeline_tag=fill-mask" class="external-link">Hugging Face</a>.</p>
<p>Use <code><a href="reference/BERT_info.html">BERT_info()</a></code> and <code><a href="reference/BERT_vocab.html">BERT_vocab()</a></code> to obtain detailed information of BERT models.</p>
</div>
<div class="section level3">
<h3 id="step-2-design-fmat-queries">Step 2: Design FMAT Queries<a class="anchor" aria-label="anchor" href="#step-2-design-fmat-queries"></a>
</h3>
<p>Design queries that conceptually represent the constructs you would measure (see <a href="https://doi.org/10.1037/pspa0000396" class="external-link">Bao, 2024, <em>JPSP</em></a> for how to design queries).</p>
<p>Use <code><a href="reference/FMAT_query.html">FMAT_query()</a></code> and/or <code><a href="reference/FMAT_query_bind.html">FMAT_query_bind()</a></code> to prepare a <code>data.table</code> of queries.</p>
</div>
<div class="section level3">
<h3 id="step-3-run-fmat">Step 3: Run FMAT<a class="anchor" aria-label="anchor" href="#step-3-run-fmat"></a>
</h3>
<p>Use <code><a href="reference/FMAT_run.html">FMAT_run()</a></code> to get raw data (probability estimates) for further analysis.</p>
<p>Several steps of preprocessing have been included in the function for easier use (see <code><a href="reference/FMAT_run.html">FMAT_run()</a></code> for details).</p>
<ul>
<li>For BERT variants using <code>&lt;mask&gt;</code> rather than <code>[MASK]</code> as the mask token, the input query will be <em>automatically</em> modified so that users can always use <code>[MASK]</code> in query design.</li>
<li>For some BERT variants, special prefix characters such as <code>\u0120</code> and <code>\u2581</code> will be <em>automatically</em> added to match the whole words (rather than subwords) for <code>[MASK]</code>.</li>
</ul>
</div>
<div class="section level3">
<h3 id="notes">Notes<a class="anchor" aria-label="anchor" href="#notes"></a>
</h3>
<ul>
<li>Improvements are ongoing, especially for adaptation to more diverse (less popular) BERT models.</li>
<li>If you find bugs or have problems using the functions, please report them at <a href="https://github.com/psychbruce/FMAT/issues" class="external-link">GitHub Issues</a> or send me an email.</li>
</ul>
</div>
</div>
<div class="section level2">
<h2 id="guidance-for-gpu-acceleration">Guidance for GPU Acceleration<a class="anchor" aria-label="anchor" href="#guidance-for-gpu-acceleration"></a>
</h2>
<p>By default, the <code>FMAT</code> package uses CPU to enable the functionality for all users. But for advanced users who want to accelerate the pipeline with GPU, the <code><a href="reference/FMAT_run.html">FMAT_run()</a></code> function now supports using a GPU device, about <strong>3x faster</strong> than CPU.</p>
<p>Test results (on the developerâ€™s computer, depending on BERT model size):</p>
<ul>
<li>CPU (Intel 13th-Gen i7-1355U): 500~1000 queries/min</li>
<li>GPU (NVIDIA GeForce RTX 2050): 1500~3000 queries/min</li>
</ul>
<p>Checklist:</p>
<ol style="list-style-type: decimal">
<li>Ensure that you have an NVIDIA GPU device (e.g., GeForce RTX Series) and an NVIDIA GPU driver installed on your system.</li>
<li>Install PyTorch (Python <code>torch</code> package) with CUDA support.
<ul>
<li>Find guidance for installation command at <a href="https://pytorch.org/get-started/locally/" class="external-link uri">https://pytorch.org/get-started/locally/</a>.</li>
<li>CUDA is available only on Windows and Linux, but not on MacOS.</li>
<li>If you have installed a version of <code>torch</code> without CUDA support, please first uninstall it (command: <code>pip uninstall torch</code>) and then install the suggested one.</li>
<li>You may also install the corresponding version of CUDA Toolkit (e.g., for the <code>torch</code> version supporting CUDA 12.1, the same version of <a href="https://developer.nvidia.com/cuda-12-1-0-download-archive" class="external-link">CUDA Toolkit 12.1</a> may also be installed).</li>
</ul>
</li>
</ol>
<p>Example code for installing PyTorch with CUDA support:<br>
(RStudio Terminal / Anaconda Prompt / Windows Command)</p>
<pre><code>pip install torch==2.2.1 --index-url https://download.pytorch.org/whl/cu121</code></pre>
</div>
<div class="section level2">
<h2 id="bert-models">BERT Models<a class="anchor" aria-label="anchor" href="#bert-models"></a>
</h2>
<p>The reliability and validity of the following 12 BERT models in the FMAT have been established in our research, but future work is needed to examine the performance of other models.</p>
<p>(model name on Hugging Face - model file size)</p>
<ol style="list-style-type: decimal">
<li>
<a href="https://huggingface.co/bert-base-uncased" class="external-link">bert-base-uncased</a> (420 MB)</li>
<li>
<a href="https://huggingface.co/bert-base-cased" class="external-link">bert-base-cased</a> (416 MB)</li>
<li>
<a href="https://huggingface.co/bert-large-uncased" class="external-link">bert-large-uncased</a> (1283 MB)</li>
<li>
<a href="https://huggingface.co/bert-large-cased" class="external-link">bert-large-cased</a> (1277 MB)</li>
<li>
<a href="https://huggingface.co/distilbert-base-uncased" class="external-link">distilbert-base-uncased</a> (256 MB)</li>
<li>
<a href="https://huggingface.co/distilbert-base-cased" class="external-link">distilbert-base-cased</a> (251 MB)</li>
<li>
<a href="https://huggingface.co/albert-base-v1" class="external-link">albert-base-v1</a> (45 MB)</li>
<li>
<a href="https://huggingface.co/albert-base-v2" class="external-link">albert-base-v2</a> (45 MB)</li>
<li>
<a href="https://huggingface.co/roberta-base" class="external-link">roberta-base</a> (476 MB)</li>
<li>
<a href="https://huggingface.co/distilroberta-base" class="external-link">distilroberta-base</a> (316 MB)</li>
<li>
<a href="https://huggingface.co/vinai/bertweet-base" class="external-link">vinai/bertweet-base</a> (517 MB)</li>
<li>
<a href="https://huggingface.co/vinai/bertweet-large" class="external-link">vinai/bertweet-large</a> (1356 MB)</li>
</ol>
<p>For details about <a href="https://arxiv.org/abs/1810.04805" class="external-link">BERT</a>, see:</p>
<ul>
<li><a href="https://huggingface.co/tasks/fill-mask" class="external-link">What is Fill-Mask? [HuggingFace]</a></li>
<li><a href="https://huggingface.co/spaces/exbert-project/exbert" class="external-link">An Explorable BERT [HuggingFace]</a></li>
<li><a href="https://huggingface.co/docs/transformers/main/en/model_doc/bert" class="external-link">BERT Model Documentation [HuggingFace]</a></li>
<li><a href="https://jalammar.github.io/illustrated-bert/" class="external-link">Illustrated BERT</a></li>
<li><a href="https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/" class="external-link">Visual Guide to BERT</a></li>
</ul>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://psychbruce.github.io/FMAT/">FMAT</a></span><span class="op">)</span></span>
<span><span class="va">models</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span></span>
<span>  <span class="st">"bert-base-uncased"</span>,</span>
<span>  <span class="st">"bert-base-cased"</span>,</span>
<span>  <span class="st">"bert-large-uncased"</span>,</span>
<span>  <span class="st">"bert-large-cased"</span>,</span>
<span>  <span class="st">"distilbert-base-uncased"</span>,</span>
<span>  <span class="st">"distilbert-base-cased"</span>,</span>
<span>  <span class="st">"albert-base-v1"</span>,</span>
<span>  <span class="st">"albert-base-v2"</span>,</span>
<span>  <span class="st">"roberta-base"</span>,</span>
<span>  <span class="st">"distilroberta-base"</span>,</span>
<span>  <span class="st">"vinai/bertweet-base"</span>,</span>
<span>  <span class="st">"vinai/bertweet-large"</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="reference/BERT_download.html">BERT_download</a></span><span class="op">(</span><span class="va">models</span><span class="op">)</span></span></code></pre></div>
<pre style="height: 500px"><code>â„¹ Device Info:

R Packages:
FMAT          2024.5
reticulate    1.36.1

Python Packages:
transformers  4.40.2
torch         2.2.1+cu121

NVIDIA GPU CUDA Support:
CUDA Enabled: TRUE
CUDA Version: 12.1
GPU (Device): NVIDIA GeForce RTX 2050


â”€â”€ Downloading model "bert-base-uncased" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ (1) Downloading configuration...
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 570/570 [00:00&lt;00:00, 114kB/s]
â†’ (2) Downloading tokenizer...
tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48.0/48.0 [00:00&lt;00:00, 23.9kB/s]
vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00&lt;00:00, 1.50MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 466k/466k [00:00&lt;00:00, 1.98MB/s]
â†’ (3) Downloading model...
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 440M/440M [00:36&lt;00:00, 12.1MB/s]
âœ” Successfully downloaded model "bert-base-uncased"

â”€â”€ Downloading model "bert-base-cased" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ (1) Downloading configuration...
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 570/570 [00:00&lt;00:00, 63.3kB/s]
â†’ (2) Downloading tokenizer...
tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49.0/49.0 [00:00&lt;00:00, 8.66kB/s]
vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 213k/213k [00:00&lt;00:00, 1.39MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436k/436k [00:00&lt;00:00, 10.1MB/s]
â†’ (3) Downloading model...
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436M/436M [00:37&lt;00:00, 11.6MB/s]
âœ” Successfully downloaded model "bert-base-cased"

â”€â”€ Downloading model "bert-large-uncased" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ (1) Downloading configuration...
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 571/571 [00:00&lt;00:00, 268kB/s]
â†’ (2) Downloading tokenizer...
tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48.0/48.0 [00:00&lt;00:00, 12.0kB/s]
vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00&lt;00:00, 1.50MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 466k/466k [00:00&lt;00:00, 1.99MB/s]
â†’ (3) Downloading model...
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.34G/1.34G [01:36&lt;00:00, 14.0MB/s]
âœ” Successfully downloaded model "bert-large-uncased"

â”€â”€ Downloading model "bert-large-cased" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ (1) Downloading configuration...
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 762/762 [00:00&lt;00:00, 125kB/s]
â†’ (2) Downloading tokenizer...
tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49.0/49.0 [00:00&lt;00:00, 12.3kB/s]
vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 213k/213k [00:00&lt;00:00, 1.41MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436k/436k [00:00&lt;00:00, 5.39MB/s]
â†’ (3) Downloading model...
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.34G/1.34G [01:35&lt;00:00, 14.0MB/s]
âœ” Successfully downloaded model "bert-large-cased"

â”€â”€ Downloading model "distilbert-base-uncased" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ (1) Downloading configuration...
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 483/483 [00:00&lt;00:00, 161kB/s]
â†’ (2) Downloading tokenizer...
tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48.0/48.0 [00:00&lt;00:00, 9.46kB/s]
vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00&lt;00:00, 16.5MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 466k/466k [00:00&lt;00:00, 14.8MB/s]
â†’ (3) Downloading model...
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268M/268M [00:19&lt;00:00, 13.5MB/s]
âœ” Successfully downloaded model "distilbert-base-uncased"

â”€â”€ Downloading model "distilbert-base-cased" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ (1) Downloading configuration...
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 465/465 [00:00&lt;00:00, 233kB/s]
â†’ (2) Downloading tokenizer...
tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49.0/49.0 [00:00&lt;00:00, 9.80kB/s]
vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 213k/213k [00:00&lt;00:00, 1.39MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436k/436k [00:00&lt;00:00, 8.70MB/s]
â†’ (3) Downloading model...
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 263M/263M [00:24&lt;00:00, 10.9MB/s]
âœ” Successfully downloaded model "distilbert-base-cased"

â”€â”€ Downloading model "albert-base-v1" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ (1) Downloading configuration...
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 684/684 [00:00&lt;00:00, 137kB/s]
â†’ (2) Downloading tokenizer...
tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25.0/25.0 [00:00&lt;00:00, 3.57kB/s]
spiece.model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 760k/760k [00:00&lt;00:00, 4.93MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.31M/1.31M [00:00&lt;00:00, 13.4MB/s]
â†’ (3) Downloading model...
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47.4M/47.4M [00:03&lt;00:00, 13.4MB/s]
âœ” Successfully downloaded model "albert-base-v1"

â”€â”€ Downloading model "albert-base-v2" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ (1) Downloading configuration...
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 684/684 [00:00&lt;00:00, 137kB/s]
â†’ (2) Downloading tokenizer...
tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25.0/25.0 [00:00&lt;00:00, 4.17kB/s]
spiece.model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 760k/760k [00:00&lt;00:00, 5.10MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.31M/1.31M [00:00&lt;00:00, 6.93MB/s]
â†’ (3) Downloading model...
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47.4M/47.4M [00:03&lt;00:00, 13.8MB/s]
âœ” Successfully downloaded model "albert-base-v2"

â”€â”€ Downloading model "roberta-base" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ (1) Downloading configuration...
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [00:00&lt;00:00, 80.3kB/s]
â†’ (2) Downloading tokenizer...
tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25.0/25.0 [00:00&lt;00:00, 6.25kB/s]
vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 899k/899k [00:00&lt;00:00, 2.72MB/s]
merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00&lt;00:00, 8.22MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.36M/1.36M [00:00&lt;00:00, 8.56MB/s]
â†’ (3) Downloading model...
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499M/499M [00:38&lt;00:00, 12.9MB/s]
âœ” Successfully downloaded model "roberta-base"

â”€â”€ Downloading model "distilroberta-base" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ (1) Downloading configuration...
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 480/480 [00:00&lt;00:00, 96.4kB/s]
â†’ (2) Downloading tokenizer...
tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25.0/25.0 [00:00&lt;00:00, 12.0kB/s]
vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 899k/899k [00:00&lt;00:00, 6.59MB/s]
merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00&lt;00:00, 9.46MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.36M/1.36M [00:00&lt;00:00, 11.5MB/s]
â†’ (3) Downloading model...
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 331M/331M [00:25&lt;00:00, 13.0MB/s]
âœ” Successfully downloaded model "distilroberta-base"

â”€â”€ Downloading model "vinai/bertweet-base" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ (1) Downloading configuration...
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 558/558 [00:00&lt;00:00, 187kB/s]
â†’ (2) Downloading tokenizer...
vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 843k/843k [00:00&lt;00:00, 7.44MB/s]
bpe.codes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.08M/1.08M [00:00&lt;00:00, 7.01MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.91M/2.91M [00:00&lt;00:00, 9.10MB/s]
â†’ (3) Downloading model...
pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 543M/543M [00:48&lt;00:00, 11.1MB/s]
âœ” Successfully downloaded model "vinai/bertweet-base"

â”€â”€ Downloading model "vinai/bertweet-large" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ (1) Downloading configuration...
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [00:00&lt;00:00, 120kB/s]
â†’ (2) Downloading tokenizer...
vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 899k/899k [00:00&lt;00:00, 5.90MB/s]
merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00&lt;00:00, 7.30MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.36M/1.36M [00:00&lt;00:00, 8.31MB/s]
â†’ (3) Downloading model...
pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.42G/1.42G [02:29&lt;00:00, 9.53MB/s]
âœ” Successfully downloaded model "vinai/bertweet-large"

â”€â”€ Downloaded models: â”€â”€

                           size
albert-base-v1            45 MB
albert-base-v2            45 MB
bert-base-cased          416 MB
bert-base-uncased        420 MB
bert-large-cased        1277 MB
bert-large-uncased      1283 MB
distilbert-base-cased    251 MB
distilbert-base-uncased  256 MB
distilroberta-base       316 MB
roberta-base             476 MB
vinai/bertweet-base      517 MB
vinai/bertweet-large    1356 MB

âœ” Downloaded models saved at C:/Users/Bruce/.cache/huggingface/hub (6.52 GB)</code></pre>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="reference/BERT_info.html">BERT_info</a></span><span class="op">(</span><span class="va">models</span><span class="op">)</span></span></code></pre></div>
<pre><code>                      model   size vocab  dims   mask
                     &lt;fctr&gt; &lt;char&gt; &lt;int&gt; &lt;int&gt; &lt;char&gt;
 1:       bert-base-uncased  420MB 30522   768 [MASK]
 2:         bert-base-cased  416MB 28996   768 [MASK]
 3:      bert-large-uncased 1283MB 30522  1024 [MASK]
 4:        bert-large-cased 1277MB 28996  1024 [MASK]
 5: distilbert-base-uncased  256MB 30522   768 [MASK]
 6:   distilbert-base-cased  251MB 28996   768 [MASK]
 7:          albert-base-v1   45MB 30000   128 [MASK]
 8:          albert-base-v2   45MB 30000   128 [MASK]
 9:            roberta-base  476MB 50265   768 &lt;mask&gt;
10:      distilroberta-base  316MB 50265   768 &lt;mask&gt;
11:     vinai/bertweet-base  517MB 64001   768 &lt;mask&gt;
12:    vinai/bertweet-large 1356MB 50265  1024 &lt;mask&gt;</code></pre>
<p>(Tested 2024-05-16 on the developerâ€™s computer: HP Probook 450 G10 Notebook PC)</p>
</div>
<div class="section level2">
<h2 id="related-packages">Related Packages<a class="anchor" aria-label="anchor" href="#related-packages"></a>
</h2>
<p>While the FMAT is an innovative method for the <em>computational intelligent</em> analysis of psychology and society, you may also seek for an integrative toolbox for other text-analytic methods. Another R package I developedâ€”<a href="https://psychbruce.github.io/PsychWordVec/" class="external-link">PsychWordVec</a>â€”is useful and user-friendly for word embedding analysis (e.g., the Word Embedding Association Test, WEAT). Please refer to its documentation and feel free to use it.</p>
</div>
</div>
  </main><aside class="col-md-3"><div class="links">
<h2 data-toc-skip>Links</h2>
<ul class="list-unstyled">
<li><a href="https://cloud.r-project.org/package=FMAT" class="external-link">View on CRAN</a></li>
<li><a href="https://github.com/psychbruce/FMAT/" class="external-link">Browse source code</a></li>
<li><a href="https://github.com/psychbruce/FMAT/issues" class="external-link">Report a bug</a></li>
</ul>
</div>

<div class="license">
<h2 data-toc-skip>License</h2>
<ul class="list-unstyled">
<li><a href="https://www.r-project.org/Licenses/GPL-3" class="external-link">GPL-3</a></li>
</ul>
</div>


<div class="citation">
<h2 data-toc-skip>Citation</h2>
<ul class="list-unstyled">
<li><a href="authors.html#citation">Citing FMAT</a></li>
</ul>
</div>

<div class="developers">
<h2 data-toc-skip>Developers</h2>
<ul class="list-unstyled">
<li>Han-Wu-Shuang Bao <br><small class="roles"> Author, maintainer </small> <a href="https://orcid.org/0000-0003-3043-710X" target="orcid.widget" aria-label="ORCID" class="external-link"><span class="fab fa-orcid orcid" aria-hidden="true"></span></a> </li>
</ul>
</div>

<div class="dev-status">
<h2 data-toc-skip>Dev status</h2>
<ul class="list-unstyled">
<li><a href="https://CRAN.R-project.org/package=FMAT" class="external-link"><img src="https://www.r-pkg.org/badges/version/FMAT?color=red" alt="CRAN-Version"></a></li>
<li><a href="https://github.com/psychbruce/FMAT" class="external-link"><img src="https://img.shields.io/github/r-package/v/psychbruce/FMAT?label=GitHub&amp;color=orange" alt="GitHub-Version"></a></li>
<li><a href="https://github.com/psychbruce/FMAT/actions/workflows/R-CMD-check.yaml" class="external-link"><img src="https://github.com/psychbruce/FMAT/actions/workflows/R-CMD-check.yaml/badge.svg" alt="R-CMD-check"></a></li>
<li><a href="https://CRAN.R-project.org/package=FMAT" class="external-link"><img src="https://cranlogs.r-pkg.org/badges/grand-total/FMAT" alt="CRAN-Downloads"></a></li>
<li><a href="https://github.com/psychbruce/FMAT/stargazers" class="external-link"><img src="https://img.shields.io/github/stars/psychbruce/FMAT?style=social" alt="GitHub-Stars"></a></li>
</ul>
</div>

  </aside>
</div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Han-Wu-Shuang Bao.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>

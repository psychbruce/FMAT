<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="
    The Fill-Mask Association Test (FMAT) &lt;doi:10.1037/pspa0000396&gt;
    is an integrative, versatile, and probability-based method
    using Masked Language Models to measure conceptual associations
    (e.g., attitudes, biases, stereotypes, social norms, cultural values)
    as propositions in natural language.
    Supported language models include BERT &lt;arXiv:1810.04805&gt;
    and its variants available at Hugging Face
    &lt;https://huggingface.co/models?pipeline_tag=fill-mask&gt;.
    Methodological references and technical details are provided at
    &lt;https://psychbruce.github.io/FMAT/&gt;,
    including guidance for installing Python (Anaconda),
    transformers and torch packages, and for using GPU acceleration.">
<title>The Fill-Mask Association Test â€¢ FMAT</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="apple-touch-icon-60x60.png">
<script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/Roboto-0.4.9/font.css" rel="stylesheet">
<link href="deps/Lexend-0.4.9/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="The Fill-Mask Association Test">
<meta property="og:description" content="
    The Fill-Mask Association Test (FMAT) &lt;doi:10.1037/pspa0000396&gt;
    is an integrative, versatile, and probability-based method
    using Masked Language Models to measure conceptual associations
    (e.g., attitudes, biases, stereotypes, social norms, cultural values)
    as propositions in natural language.
    Supported language models include BERT &lt;arXiv:1810.04805&gt;
    and its variants available at Hugging Face
    &lt;https://huggingface.co/models?pipeline_tag=fill-mask&gt;.
    Methodological references and technical details are provided at
    &lt;https://psychbruce.github.io/FMAT/&gt;,
    including guidance for installing Python (Anaconda),
    transformers and torch packages, and for using GPU acceleration.">
<meta property="og:image" content="https://psychbruce.github.io/FMAT/logo.png">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="index.html">FMAT</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">2024.3</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/psychbruce/FMAT/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-home">
<div class="row">
  <main id="main" class="col-md-9"><div class="section level1">
<div class="page-header">
<img src="logo.png" class="logo" alt=""><h1 id="fmat-">FMAT <a class="anchor" aria-label="anchor" href="#fmat-"></a>
</h1>
</div>
<p>ğŸ˜· The Fill-Mask Association Test (æ©ç å¡«ç©ºè”ç³»æµ‹éªŒ).</p>
<p>The <em>Fill-Mask Association Test</em> (FMAT) is an integrative, versatile, and probability-based method using Masked Language Models (<a href="https://arxiv.org/abs/1810.04805" class="external-link">BERT</a>) to measure conceptual associations (e.g., attitudes, biases, stereotypes, social norms, cultural values) as <em>propositions</em> in natural language (<a href="https://doi.org/10.1037/pspa0000396" class="external-link">Bao, 2024, <em>JPSP</em></a>).</p>
<div class="float">
<img src="https://psychbruce.github.io/img/FMAT-Workflow.png" alt="FMAT Workflow"><div class="figcaption">FMAT Workflow</div>
</div>
<!-- badges: start -->

<!-- badges: end -->
<p><img src="https://psychbruce.github.io/img/CC-BY-NC-SA.jpg" width="120px" height="42px"></p>
<div class="section level2">
<h2 id="author">Author<a class="anchor" aria-label="anchor" href="#author"></a>
</h2>
<p>Han-Wu-Shuang (Bruce) Bao åŒ…å¯’å´éœœ</p>
<p>ğŸ“¬ <a href="mailto:baohws@foxmail.com">baohws@foxmail.com</a></p>
<p>ğŸ“‹ <a href="https://psychbruce.github.io" class="external-link">psychbruce.github.io</a></p>
</div>
<div class="section level2">
<h2 id="citation">Citation<a class="anchor" aria-label="anchor" href="#citation"></a>
</h2>
<ul>
<li>Bao, H.-W.-S. (2023). <em>FMAT: The Fill-Mask Association Test</em>. <a href="https://CRAN.R-project.org/package=FMAT" class="external-link uri">https://CRAN.R-project.org/package=FMAT</a>
<ul>
<li>
<em>Note</em>: This is the original citation format. Please refer to the information when you <code><a href="https://psychbruce.github.io/FMAT/">library(FMAT)</a></code> for the APA-7 format of your installed version.</li>
</ul>
</li>
<li>Bao, H.-W.-S. (in press). The Fill-Mask Association Test (FMAT): Measuring propositions in natural language. <em>Journal of Personality and Social Psychology</em>. <a href="https://doi.org/10.1037/pspa0000396" class="external-link uri">https://doi.org/10.1037/pspa0000396</a>
</li>
</ul>
</div>
<div class="section level2">
<h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h2>
<p>To use the FMAT, the R package <code>FMAT</code> and two Python packages (<code>transformers</code> and <code>torch</code>) all need to be installed.</p>
<div class="section level3">
<h3 id="id_1-r-package">(1) R Package<a class="anchor" aria-label="anchor" href="#id_1-r-package"></a>
</h3>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## Method 1: Install from CRAN</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"FMAT"</span><span class="op">)</span></span>
<span></span>
<span><span class="co">## Method 2: Install from GitHub</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html" class="external-link">install.packages</a></span><span class="op">(</span><span class="st">"devtools"</span><span class="op">)</span></span>
<span><span class="fu">devtools</span><span class="fu">::</span><span class="fu">install_github</span><span class="op">(</span><span class="st">"psychbruce/FMAT"</span>, force<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="id_2-python-environment-and-packages">(2) Python Environment and Packages<a class="anchor" aria-label="anchor" href="#id_2-python-environment-and-packages"></a>
</h3>
<div class="section level4">
<h4 id="step-1">Step 1<a class="anchor" aria-label="anchor" href="#step-1"></a>
</h4>
<p>Install <a href="https://www.anaconda.com/download" class="external-link">Anaconda</a> (a recommended package manager which automatically installs Python, Python IDEs like Spyder, and a large list of necessary <a href="https://docs.anaconda.com/free/anaconda/pkg-docs/" class="external-link">Python package dependencies</a>).</p>
</div>
<div class="section level4">
<h4 id="step-2">Step 2<a class="anchor" aria-label="anchor" href="#step-2"></a>
</h4>
<p>Specify the Python interpreter in RStudio.</p>
<blockquote>
<p>RStudio â†’ Tools â†’ Global/Project Options<br>
â†’ Python â†’ Select â†’ <strong>Conda Environments</strong><br>
â†’ Choose <strong>â€œâ€¦/Anaconda3/python.exeâ€</strong></p>
</blockquote>
</div>
<div class="section level4">
<h4 id="step-3">Step 3<a class="anchor" aria-label="anchor" href="#step-3"></a>
</h4>
<p>Install the â€œ<a href="https://huggingface.co/docs/transformers/installation" class="external-link">transformers</a>â€ and â€œ<a href="https://pytorch.org/get-started/locally/" class="external-link">torch</a>â€ Python packages.<br>
(Windows Command / Anaconda Prompt / RStudio Terminal)</p>
<pre><code>pip install transformers torch</code></pre>
<p>See <a href="#guidance-for-gpu-acceleration">Guidance for GPU Acceleration</a> if you have an NVIDIA GPU device on your PC and want to use GPU to accelerate the pipeline.</p>
</div>
<div class="section level4">
<h4 id="alternative-approach">Alternative Approach<a class="anchor" aria-label="anchor" href="#alternative-approach"></a>
</h4>
<p>(Not suggested) Besides the pip/conda installation in the <em>Conda Environment</em>, you might instead create and use a <em>Virtual Environment</em> (see R code below with the <code>reticulate</code> package), but then you need to specify the Python interpreter as <strong>â€œ~/.virtualenvs/r-reticulate/Scripts/python.exeâ€</strong> in RStudio.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">## DON'T RUN THIS UNLESS YOU PREFER VIRTUAL ENVIRONMENT</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://rstudio.github.io/reticulate/" class="external-link">reticulate</a></span><span class="op">)</span></span>
<span><span class="co"># install_python()</span></span>
<span><span class="fu"><a href="https://rstudio.github.io/reticulate/reference/virtualenv-tools.html" class="external-link">virtualenv_create</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rstudio.github.io/reticulate/reference/virtualenv-tools.html" class="external-link">virtualenv_install</a></span><span class="op">(</span>packages<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"transformers"</span>, <span class="st">"torch"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
</div>
<div class="section level2">
<h2 id="guidance-for-fmat">Guidance for FMAT<a class="anchor" aria-label="anchor" href="#guidance-for-fmat"></a>
</h2>
<div class="section level3">
<h3 id="fmat-step-1-query-design">FMAT Step 1: Query Design<a class="anchor" aria-label="anchor" href="#fmat-step-1-query-design"></a>
</h3>
<p>Design queries that conceptually represent the constructs you would measure (see <a href="https://doi.org/10.1037/pspa0000396" class="external-link">Bao, 2024, <em>JPSP</em></a> for how to design queries).</p>
<p>Use <code><a href="reference/FMAT_query.html">FMAT_query()</a></code> and/or <code><a href="reference/FMAT_query_bind.html">FMAT_query_bind()</a></code> to prepare a <code>data.table</code> of queries.</p>
</div>
<div class="section level3">
<h3 id="fmat-step-2-model-loading">FMAT Step 2: Model Loading<a class="anchor" aria-label="anchor" href="#fmat-step-2-model-loading"></a>
</h3>
<p>Use <code><a href="reference/BERT_download.html">BERT_download()</a></code> and <code><a href="reference/FMAT_load.html">FMAT_load()</a></code> to (down)load <a href="#bert-models">BERT models</a>. Model files are saved to your local folder â€œ%USERPROFILE%/.cache/huggingfaceâ€. A full list of BERT-family models are available at <a href="https://huggingface.co/models?pipeline_tag=fill-mask&amp;library=transformers" class="external-link">Hugging Face</a>.</p>
</div>
<div class="section level3">
<h3 id="fmat-step-3-model-processing">FMAT Step 3: Model Processing<a class="anchor" aria-label="anchor" href="#fmat-step-3-model-processing"></a>
</h3>
<p>Use <code><a href="reference/FMAT_run.html">FMAT_run()</a></code> to get raw data (probability estimates) for further analysis.</p>
<p>Several steps of pre-processing have been included in the function for easier use (see <code><a href="reference/FMAT_run.html">FMAT_run()</a></code> for details).</p>
<ul>
<li>For BERT variants using <code>&lt;mask&gt;</code> rather than <code>[MASK]</code> as the mask token, the input query will be <em>automatically</em> modified so that users can always use <code>[MASK]</code> in query design.</li>
<li>For some BERT variants, special prefix characters such as <code>\u0120</code> and <code>\u2581</code> will be <em>automatically</em> added to match the whole words (rather than subwords) for <code>[MASK]</code>.</li>
</ul>
</div>
<div class="section level3">
<h3 id="notes">Notes<a class="anchor" aria-label="anchor" href="#notes"></a>
</h3>
<ul>
<li>Improvements are ongoing, especially for adaptation to more diverse (less popular) BERT models.</li>
<li>If you find bugs or have problems using the functions, please report them at <a href="https://github.com/psychbruce/FMAT/issues" class="external-link">GitHub Issues</a> or send me an email.</li>
</ul>
</div>
</div>
<div class="section level2">
<h2 id="guidance-for-gpu-acceleration">Guidance for GPU Acceleration<a class="anchor" aria-label="anchor" href="#guidance-for-gpu-acceleration"></a>
</h2>
<div class="section level3">
<h3 id="nvidia-gpu-acceleration">NVIDIA GPU Acceleration<a class="anchor" aria-label="anchor" href="#nvidia-gpu-acceleration"></a>
</h3>
<p>By default, the <code>FMAT</code> package uses CPU to enable the functionality for all users. But for advanced users who want to accelerate the pipeline with GPU, the <code><a href="reference/FMAT_load.html">FMAT_load()</a></code> function now supports using a GPU device, which may perform <strong>3x faster</strong> than CPU.</p>
</div>
<div class="section level3">
<h3 id="step-1-1">Step 1<a class="anchor" aria-label="anchor" href="#step-1-1"></a>
</h3>
<p>Ensure that you have an NVIDIA GPU device (e.g., GeForce RTX Series) and an NVIDIA GPU driver installed on your system.</p>
</div>
<div class="section level3">
<h3 id="step-2-1">Step 2<a class="anchor" aria-label="anchor" href="#step-2-1"></a>
</h3>
<p>Install PyTorch (Python <code>torch</code> package) with CUDA support (<a href="https://pytorch.org/get-started/locally/" class="external-link uri">https://pytorch.org/get-started/locally/</a>).</p>
<ul>
<li>CUDA is only available on Windows and Linux, but not on MacOS.</li>
<li>If you have installed a version of <code>torch</code> without CUDA support, please first uninstall it (command: <code>pip uninstall torch</code>) and then install the suggested one.</li>
<li>You may also install the corresponding version of CUDA Toolkit (e.g., for the <code>torch</code> version supporting CUDA 12.1, the same version of <a href="https://developer.nvidia.com/cuda-12-1-0-download-archive" class="external-link">CUDA Toolkit 12.1</a> may also be installed).</li>
</ul>
<p>Example code for installing PyTorch with CUDA support:<br>
(Windows Command / Anaconda Prompt / RStudio Terminal)</p>
<pre><code>pip install torch --index-url https://download.pytorch.org/whl/cu121</code></pre>
</div>
<div class="section level3">
<h3 id="step-3-1">Step 3<a class="anchor" aria-label="anchor" href="#step-3-1"></a>
</h3>
<p>Check with the <code>FMAT</code> package.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://psychbruce.github.io/FMAT/">FMAT</a></span><span class="op">)</span></span>
<span><span class="co"># BERT_download("bert-base-uncased")</span></span>
<span><span class="va">model</span> <span class="op">=</span> <span class="fu"><a href="reference/FMAT_load.html">FMAT_load</a></span><span class="op">(</span><span class="st">"bert-base-uncased"</span>, gpu<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<pre><code>â„¹ Device Info:

Python Environment:
Package       Version
transformers  4.38.2
torch         2.2.1+cu121

NVIDIA GPU CUDA Support:
CUDA Enabled: TRUE
CUDA Version: 12.1
GPU (Device): NVIDIA GeForce RTX 2050

Loading models from C:/Users/Bruce/.cache/huggingface/hub...
âœ” bert-base-uncased (1.1s) - GPU (device id = 0)</code></pre>
<p>(Tested 2024/03 on the developerâ€™s computer: HP Probook 450 G10 Notebook PC)</p>
</div>
</div>
<div class="section level2">
<h2 id="bert-models">BERT Models<a class="anchor" aria-label="anchor" href="#bert-models"></a>
</h2>
<p>The reliability and validity of the following 12 representative BERT models have been established in my research articles, but future work is needed to examine the performance of other models.</p>
<p>(model name on Hugging Face - downloaded model file size)</p>
<ol style="list-style-type: decimal">
<li>
<a href="https://huggingface.co/bert-base-uncased" class="external-link">bert-base-uncased</a> (420 MB)</li>
<li>
<a href="https://huggingface.co/bert-base-cased" class="external-link">bert-base-cased</a> (416 MB)</li>
<li>
<a href="https://huggingface.co/bert-large-uncased" class="external-link">bert-large-uncased</a> (1283 MB)</li>
<li>
<a href="https://huggingface.co/bert-large-cased" class="external-link">bert-large-cased</a> (1277 MB)</li>
<li>
<a href="https://huggingface.co/distilbert-base-uncased" class="external-link">distilbert-base-uncased</a> (256 MB)</li>
<li>
<a href="https://huggingface.co/distilbert-base-cased" class="external-link">distilbert-base-cased</a> (251 MB)</li>
<li>
<a href="https://huggingface.co/albert-base-v1" class="external-link">albert-base-v1</a> (45 MB)</li>
<li>
<a href="https://huggingface.co/albert-base-v2" class="external-link">albert-base-v2</a> (45 MB)</li>
<li>
<a href="https://huggingface.co/roberta-base" class="external-link">roberta-base</a> (476 MB)</li>
<li>
<a href="https://huggingface.co/distilroberta-base" class="external-link">distilroberta-base</a> (316 MB)</li>
<li>
<a href="https://huggingface.co/vinai/bertweet-base" class="external-link">vinai/bertweet-base</a> (517 MB)</li>
<li>
<a href="https://huggingface.co/vinai/bertweet-large" class="external-link">vinai/bertweet-large</a> (1356 MB)</li>
</ol>
<p>If you are new to <a href="https://arxiv.org/abs/1810.04805" class="external-link">BERT</a>, please read:</p>
<ul>
<li><a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270" class="external-link">BERT Explained</a></li>
<li><a href="https://towardsdatascience.com/breaking-bert-down-430461f60efb" class="external-link">Breaking BERT Down</a></li>
<li><a href="https://jalammar.github.io/illustrated-bert/" class="external-link">Illustrated BERT</a></li>
<li><a href="https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/" class="external-link">Visual Guide to BERT</a></li>
<li><a href="https://huggingface.co/docs/transformers/main/en/model_doc/bert" class="external-link">BERT Model Documentation</a></li>
<li><a href="https://huggingface.co/tasks/fill-mask" class="external-link">What is Fill-Mask?</a></li>
</ul>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://psychbruce.github.io/FMAT/">FMAT</a></span><span class="op">)</span></span>
<span><span class="va">model.names</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span></span>
<span>  <span class="st">"bert-base-uncased"</span>,</span>
<span>  <span class="st">"bert-base-cased"</span>,</span>
<span>  <span class="st">"bert-large-uncased"</span>,</span>
<span>  <span class="st">"bert-large-cased"</span>,</span>
<span>  <span class="st">"distilbert-base-uncased"</span>,</span>
<span>  <span class="st">"distilbert-base-cased"</span>,</span>
<span>  <span class="st">"albert-base-v1"</span>,</span>
<span>  <span class="st">"albert-base-v2"</span>,</span>
<span>  <span class="st">"roberta-base"</span>,</span>
<span>  <span class="st">"distilroberta-base"</span>,</span>
<span>  <span class="st">"vinai/bertweet-base"</span>,</span>
<span>  <span class="st">"vinai/bertweet-large"</span></span>
<span><span class="op">)</span></span>
<span><span class="fu"><a href="reference/BERT_download.html">BERT_download</a></span><span class="op">(</span><span class="va">model.names</span><span class="op">)</span></span></code></pre></div>
<pre><code>â„¹ Device Info:

Python Environment:
Package       Version
transformers  4.38.2
torch         2.2.1+cu121

NVIDIA GPU CUDA Support:
CUDA Enabled: TRUE
CUDA Version: 12.1
GPU (Device): NVIDIA GeForce RTX 2050


â”€â”€ Downloading model "bert-base-uncased" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Downloading configuration...
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 570/570 [00:00&lt;00:00, 113kB/s]
Downloading tokenizer...
tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48.0/48.0 [00:00&lt;?, ?B/s]
vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00&lt;00:00, 1.37MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 466k/466k [00:00&lt;00:00, 3.94MB/s]
Downloading model...
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 440M/440M [01:21&lt;00:00, 5.40MB/s] 
âœ” Successfully downloaded model "bert-base-uncased"

â”€â”€ Downloading model "bert-base-cased" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Downloading configuration...
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 570/570 [00:00&lt;?, ?B/s] 
Downloading tokenizer...
tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49.0/49.0 [00:00&lt;00:00, 8.18kB/s]
vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 213k/213k [00:00&lt;00:00, 1.30MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436k/436k [00:00&lt;00:00, 3.67MB/s]
Downloading model...
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436M/436M [01:20&lt;00:00, 5.41MB/s] 
âœ” Successfully downloaded model "bert-base-cased"

â”€â”€ Downloading model "bert-large-uncased" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Downloading configuration...
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 571/571 [00:00&lt;00:00, 143kB/s]
Downloading tokenizer...
tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48.0/48.0 [00:00&lt;00:00, 12.0kB/s]
vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00&lt;00:00, 6.04MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 466k/466k [00:00&lt;00:00, 1.57MB/s]
Downloading model...
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.34G/1.34G [04:09&lt;00:00, 5.39MB/s]
âœ” Successfully downloaded model "bert-large-uncased"

â”€â”€ Downloading model "bert-large-cased" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Downloading configuration...
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 762/762 [00:00&lt;?, ?B/s] 
Downloading tokenizer...
tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49.0/49.0 [00:00&lt;?, ?B/s]
vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 213k/213k [00:00&lt;00:00, 2.14MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436k/436k [00:00&lt;00:00, 1.75MB/s]
Downloading model...
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.34G/1.34G [04:08&lt;00:00, 5.38MB/s]
âœ” Successfully downloaded model "bert-large-cased"

â”€â”€ Downloading model "distilbert-base-uncased" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Downloading configuration...
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 483/483 [00:00&lt;?, ?B/s] 
Downloading tokenizer...
tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.0/28.0 [00:00&lt;?, ?B/s]
vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00&lt;00:00, 1.36MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 466k/466k [00:00&lt;00:00, 1.82MB/s]
Downloading model...
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268M/268M [00:51&lt;00:00, 5.24MB/s] 
âœ” Successfully downloaded model "distilbert-base-uncased"

â”€â”€ Downloading model "distilbert-base-cased" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Downloading configuration...
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 465/465 [00:00&lt;?, ?B/s] 
Downloading tokenizer...
tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29.0/29.0 [00:00&lt;?, ?B/s]
vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 213k/213k [00:00&lt;00:00, 1.34MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 436k/436k [00:00&lt;00:00, 4.20MB/s]
Downloading model...
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 263M/263M [00:49&lt;00:00, 5.36MB/s] 
âœ” Successfully downloaded model "distilbert-base-cased"

â”€â”€ Downloading model "albert-base-v1" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Downloading configuration...
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 684/684 [00:00&lt;?, ?B/s] 
Downloading tokenizer...
tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25.0/25.0 [00:00&lt;00:00, 1.65kB/s]
spiece.model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 760k/760k [00:00&lt;00:00, 4.58MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.31M/1.31M [00:00&lt;00:00, 3.09MB/s]
Downloading model...
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47.4M/47.4M [00:09&lt;00:00, 5.07MB/s]
âœ” Successfully downloaded model "albert-base-v1"

â”€â”€ Downloading model "albert-base-v2" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Downloading configuration...
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 684/684 [00:00&lt;00:00, 45.5kB/s]
Downloading tokenizer...
tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25.0/25.0 [00:00&lt;?, ?B/s]
spiece.model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 760k/760k [00:00&lt;00:00, 2.13MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.31M/1.31M [00:00&lt;00:00, 5.66MB/s]
Downloading model...
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47.4M/47.4M [00:08&lt;00:00, 5.51MB/s]
âœ” Successfully downloaded model "albert-base-v2"

â”€â”€ Downloading model "roberta-base" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Downloading configuration...
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [00:00&lt;?, ?B/s] 
Downloading tokenizer...
tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25.0/25.0 [00:00&lt;?, ?B/s]
vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 899k/899k [00:00&lt;00:00, 5.73MB/s]
merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00&lt;00:00, 6.16MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.36M/1.36M [00:00&lt;00:00, 5.50MB/s]
Downloading model...
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 499M/499M [01:32&lt;00:00, 5.38MB/s] 
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
âœ” Successfully downloaded model "roberta-base"

â”€â”€ Downloading model "distilroberta-base" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Downloading configuration...
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 480/480 [00:00&lt;00:00, 30.7kB/s]
Downloading tokenizer...
tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25.0/25.0 [00:00&lt;00:00, 7.98kB/s]
vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 899k/899k [00:00&lt;00:00, 5.18MB/s]
merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00&lt;00:00, 5.71MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.36M/1.36M [00:00&lt;00:00, 3.83MB/s]
Downloading model...
model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 331M/331M [01:01&lt;00:00, 5.39MB/s] 
âœ” Successfully downloaded model "distilroberta-base"

â”€â”€ Downloading model "vinai/bertweet-base" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Downloading configuration...
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 558/558 [00:00&lt;?, ?B/s] 
Downloading tokenizer...
vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 843k/843k [00:00&lt;00:00, 5.56MB/s]
bpe.codes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.08M/1.08M [00:00&lt;00:00, 5.55MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.91M/2.91M [00:00&lt;00:00, 5.50MB/s]
emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0
Downloading model...
pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 543M/543M [01:40&lt;00:00, 5.39MB/s] 
âœ” Successfully downloaded model "vinai/bertweet-base"

â”€â”€ Downloading model "vinai/bertweet-large" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Downloading configuration...
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 614/614 [00:00&lt;?, ?B/s] 
Downloading tokenizer...
vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 899k/899k [00:00&lt;00:00, 5.59MB/s]
merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00&lt;00:00, 5.04MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.36M/1.36M [00:00&lt;00:00, 5.42MB/s]
Downloading model...
pytorch_model.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.42G/1.42G [04:23&lt;00:00, 5.40MB/s]
Some weights of RobertaModel were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
âœ” Successfully downloaded model "vinai/bertweet-large"

â”€â”€ Downloaded models: â”€â”€

                           Size
albert-base-v1            45 MB
albert-base-v2            45 MB
bert-base-cased          416 MB
bert-base-uncased        420 MB
bert-large-cased        1277 MB
bert-large-uncased      1283 MB
distilbert-base-cased    251 MB
distilbert-base-uncased  256 MB
distilroberta-base       316 MB
roberta-base             476 MB
vinai/bertweet-base      517 MB
vinai/bertweet-large    1356 MB

âœ” Downloaded models saved at C:/Users/Bruce/.cache/huggingface/hub (6.52 GB)</code></pre>
<p>(Tested 2024/03 on the developerâ€™s computer: HP Probook 450 G10 Notebook PC)</p>
</div>
<div class="section level2">
<h2 id="related-packages">Related Packages<a class="anchor" aria-label="anchor" href="#related-packages"></a>
</h2>
<p>While the FMAT is an innovative method for the <em>computational intelligent</em> analysis of psychology and society, you may also seek for an integrative toolbox for other text-analytic methods. Another R package I developedâ€”<a href="https://psychbruce.github.io/PsychWordVec/" class="external-link">PsychWordVec</a>â€”is useful and user-friendly for word embedding analysis (e.g., the Word Embedding Association Test, WEAT). Please refer to its documentation and feel free to use it.</p>
</div>
</div>
  </main><aside class="col-md-3"><div class="links">
<h2 data-toc-skip>Links</h2>
<ul class="list-unstyled">
<li><a href="https://cloud.r-project.org/package=FMAT" class="external-link">View on CRAN</a></li>
<li><a href="https://github.com/psychbruce/FMAT/" class="external-link">Browse source code</a></li>
<li><a href="https://github.com/psychbruce/FMAT/issues" class="external-link">Report a bug</a></li>
</ul>
</div>

<div class="license">
<h2 data-toc-skip>License</h2>
<ul class="list-unstyled">
<li><a href="https://www.r-project.org/Licenses/GPL-3" class="external-link">GPL-3</a></li>
</ul>
</div>


<div class="citation">
<h2 data-toc-skip>Citation</h2>
<ul class="list-unstyled">
<li><a href="authors.html#citation">Citing FMAT</a></li>
</ul>
</div>

<div class="developers">
<h2 data-toc-skip>Developers</h2>
<ul class="list-unstyled">
<li>Han-Wu-Shuang Bao <br><small class="roles"> Author, maintainer </small> <a href="https://orcid.org/0000-0003-3043-710X" target="orcid.widget" aria-label="ORCID" class="external-link"><span class="fab fa-orcid orcid" aria-hidden="true"></span></a> </li>
</ul>
</div>

<div class="dev-status">
<h2 data-toc-skip>Dev status</h2>
<ul class="list-unstyled">
<li><a href="https://CRAN.R-project.org/package=FMAT" class="external-link"><img src="https://www.r-pkg.org/badges/version/FMAT?color=red" alt="CRAN-Version"></a></li>
<li><a href="https://github.com/psychbruce/FMAT" class="external-link"><img src="https://img.shields.io/github/r-package/v/psychbruce/FMAT?label=GitHub&amp;color=orange" alt="GitHub-Version"></a></li>
<li><a href="https://github.com/psychbruce/FMAT/actions/workflows/R-CMD-check.yaml" class="external-link"><img src="https://github.com/psychbruce/FMAT/actions/workflows/R-CMD-check.yaml/badge.svg" alt="R-CMD-check"></a></li>
<li><a href="https://CRAN.R-project.org/package=FMAT" class="external-link"><img src="https://cranlogs.r-pkg.org/badges/grand-total/FMAT" alt="CRAN-Downloads"></a></li>
<li><a href="https://github.com/psychbruce/FMAT/stargazers" class="external-link"><img src="https://img.shields.io/github/stars/psychbruce/FMAT?style=social" alt="GitHub-Stars"></a></li>
</ul>
</div>

  </aside>
</div>


    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Han-Wu-Shuang Bao.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>

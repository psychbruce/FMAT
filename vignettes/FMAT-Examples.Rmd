---
title: "FMAT: Basic Principles and Examples"
author: "Han-Wu-Shuang Bao"
date: "`r Sys.Date()`"
output:
  html_vignette:
    toc: true
    toc_depth: 2
    anchor_sections: true
    df_print: kable
vignette: >
  %\VignetteIndexEntry{FMAT: Basic Principles and Examples}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{=html}
<style type="text/css">
  body { font-family: "PT Sans", "Noto Sans", Arial, sans-serif;
         font-size: 14px; color: #000; }
  #header { text-align: center; }
  h1, h2, h3 { font-weight: bold; }
  h1.title { font-size: 34px; }
  h1 { font-size: 32px; }
  h2 { font-size: 28px; }
  h3 { font-size: 24px; }
  h4 { font-size: 20px; }
  #TOC li { font-size: 18px; line-height: 1.25; }
  p, li, button span { font-size: 16px; }
  table th, table td { border-width: 0px; line-height: 1.2; }
  table thead { background-color: #f0f0f0; }
  table tr.even { background-color: #f5f5f5; }
  pre code { font-family: "Consolas", "Monaco", monospace; }
  pre code span.do { font-weight: bold; }
  pre code span.fu { color: #8959a8; }
</style>
```
```{r Config, include=FALSE}
options(
  knitr.kable.NA = "",
  digits = 3
)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 6,
  fig.height = 4,
  dpi = 100
)
```

# Basic Principles

The **FMAT (Fill-Mask Association Test)** is a generic, probability-based method that employs masked language models (e.g., BERT) to measure conceptual associations. In contrast with the use of *cosine similarity* in the WEAT (Word-Embedding Association Test; Caliskan et al., 2017), the FMAT calculates the *log probability* of specific words that are likely to replace the blank `[MASK]` in a finely-designed contextualized query.

For example, in the query

    Paris is the [MASK] of France.

The conditional probability of a specific word (e.g., `"capital"`) to replace the `[MASK]` token, given this context, will be estimated by a masked language model (the probabilities of all words in the model vocabulary will add up to 100%).

$$
\text{Conditional Probability} = P(w | context_w)
$$

The log-transformation of the probability will then enable its linear addition and subtraction. The index used in the FMAT is the **log probability ratio (LPR)**, which are typically contrasted between attributes (*A* vs. *B*) and between targets (*X* vs. *Y*).

$$
\text{LPR}(w)_{a \text{ vs } b} = \log \frac {P(w|context_a)} {P(w|context_b)} = \log P(w|context_a) - \log P(w|context_b)
$$

$$
\text{LPR}(w)_{A \text{ vs } B} = \text{mean}_{a \in A} \log P(w|context_a) - \text{mean}_{b \in B} \log P(w|context_b)
$$

$$
\text{FMAT}(X \text{ vs } Y)_{A \text{ vs } B} = \text{mean}_{x \in X} \text{LPR}(x)_{A \text{ vs } B} - \text{mean}_{y \in Y} \text{LPR}(y)_{A \text{ vs } B}
$$

The FMAT is flexible in design, but is also subject to the following basic principles in practice:

1.  `[MASK]`, `{TARGET}`, `{ATTRIB}` should all be *named* lists. For example, `list(Name = c("item1", "item2", "item3"))`, or its equivalent simplified version `.(Name = s("item1, item2, item3"))`. The names will be used as the labels/levels of the variables.
2.  `[MASK]`, `{TARGET}`, `{ATTRIB}` may be either *pairwise* (items will be pairwise contrasted and the order matters) or *listwise* (item order does not matter). However, even in the case of listwise, the scores (both probability and LPR) should always be contrasted with a reference level/group and interpreted as relative (vs. absolute) associations.
3.  `[MASK]` should always be included in `query`, since the FMAT is based on the *masked language models*. The target words of `[MASK]` must be in the model vocabulary; otherwise, users may consider using `{TARGET}` and/or `{ATTRIB}`, which can be specified as out-of-vocabulary words or any phrases (much more flexible than `[MASK]`).
4.  If only `{TARGET}` or `{ATTRIB}` (not both) is specified, they operate in exactly the same way (see Ex.2, where `TARGET` and `ATTRIB` are actually interchangeable).
5.  If both `{TARGET}` and `{ATTRIB}` are specified, both of them must be (and will be forced to) *listwise*; that is, all combinations of the targets and the attributes will be produced and each combination will be used for one query. This is the generic design of the FMAT. Then, the `summary()` function will produce the *mean difference of LPR* for each target word, with the mean log probability averaged across all attribute words within one level of attributes (see Ex.3, where `TARGET` and `ATTRIB` are treated differently).

> Regarding the correspondence between the FMAT and the WEAT/WEFAT (Caliskan et al., 2017), a conceptual replication of WEFAT or SC-WEAT (single-category target × two attributes) requires the design of "`[MASK]` (pairwise) + `{TARGET}` (listwise)" (see Ex.2b), whereas a conceptual replication of WEAT (two targets × two attributes) requires the generic design of "`[MASK]` (pairwise) + `{TARGET}` (listwise) + `{ATTRIB}` (listwise)" (see Ex.3a). In this way, we see that the WEAT and the WEFAT/SC-WEAT may be understood as word-level special cases of the FMAT.

# Examples

## Setup

```{r Setup, message=FALSE}
## Load R packages

library(FMAT)
library(nlme)
library(knitr)
library(bruceR)

## Default contrast method used in model fitting
## "contr.sum" is necessary for accurate results of main effects in ANOVA
options(contrasts=c("contr.sum", "contr.poly"))
```

## Load BERT Models

```{r Load}
models = FMAT_load(c("bert-base-uncased", "bert-base-cased"))
```

Note that these two BERT models are only the most popular and basic ones among all the BERT-based language models available from Hugging Face. For a full list of models that can be used in the FMAT, please see Hugging Face: <https://huggingface.co/models?pipeline_tag=fill-mask&library=transformers>

```{r Models, echo=FALSE}
read.csv("https://psychbruce.github.io/HuggingFace_Models.csv", encoding="UTF-8")
```

\* *Note*. The accuracy of the FMAT results is subject to the selected models, the pre-training corpora, and the design of queries, but not to the FMAT methodology.

Abbreviations: wiki = [Wikipedia](https://huggingface.co/datasets/wikipedia); book = [BookCorpus](https://huggingface.co/datasets/bookcorpus); cc = [CommonCrawl](https://commoncrawl.org/); open = [OpenWebText](https://github.com/jcpeterson/openwebtext).

## Ex.1: `[MASK]` only

### Ex.1a: `[MASK]` (pairwise)

```{r Ex1a}
## FMAT
q1a = FMAT_query(
  c("[MASK] is a nurse.", "[MASK] works as a nurse."),
  MASK = .(Male="He", Female="She")
)
d1a = FMAT_run(models, q1a)
summary(d1a)
```

```{r Ex1a Results, message=FALSE}
## Linear Model
## contrasts: contr.sum
lm = lm(LPR ~ MASK * model, data=summary(d1a, mask.pair=FALSE))
car::Anova(lm, type=3) %>% kable(digits=6)
emmeans(lm, "MASK") %>% contrast(method="pairwise") %>% kable(digits=6)
```

### Ex.1b: `[MASK]` (listwise)

```{r Ex1b}
## FMAT
q1b = FMAT_query(
  c("[MASK] is a nurse.", "[MASK] works as a nurse."),
  MASK = .(Male=cc("Albert, Bruce, Charles, Daniel, Smith"),
           Female=cc("Alice, Betty, Chloe, Daisy"))
)
d1b = FMAT_run(models, q1b)
summary(d1b, mask.pair=FALSE)
```

```{r Ex1b Results, message=FALSE}
## Linear Model
## contrasts: contr.sum
lm = lm(LPR ~ MASK * model, data=summary(d1b, mask.pair=FALSE))
car::Anova(lm, type=3) %>% kable(digits=6)
emmeans(lm, "MASK") %>% contrast(method="pairwise") %>% kable(digits=6)
## Linear Mixed Model
## (more conservative and suitable for large number of random items)
lmm = lme(LPR ~ MASK * model,
          random = ~ 1 | M_word,
          data=summary(d1b, mask.pair=FALSE))
anova(lmm) %>% kable()
emmeans(lmm, "MASK") %>% contrast(method="pairwise") %>% kable()
```

## Ex.2: `[MASK]` + `{TARGET}`/`{ATTRIB}`

In this case (i.e., `[MASK]` + one `{...}`), `{TARGET}` and `{ATTRIB}` are *interchangeable* and will operate in exactly the same way. Both of them can be either pairwise or listwise. Here are two illustrative examples for the cases of pairwise (using `{ATTRIB}` as an example) and listwise (using `{TARGET}` as an example).

### Ex.2a: `[MASK]` (pairwise) + `{ATTRIB}` (pairwise)

```{r Ex2a}
## FMAT
q2a = FMAT_query(
  "The [MASK] {ATTRIB}.",
  MASK = .(Male=cc("man, boy"), Female=cc("woman, girl")),
  ATTRIB = .(Masc=cc("is masculine, has a masculine personality"),
             Femi=cc("is feminine, has a feminine personality"))
)
d2a = FMAT_run(models, q2a)
summary(d2a)
```

```{r Ex2a Results, message=FALSE}
## Linear Model
## contrasts: contr.sum
lm = lm(LPR ~ MASK * model, data=summary(d2a, mask.pair=FALSE))
car::Anova(lm, type=3) %>% kable()
emmeans(lm, "MASK") %>% contrast(method="pairwise") %>% kable()
```

### Ex.2b: `[MASK]` (pairwise) + `{TARGET}` (listwise)

```{r Ex2b}
## FMAT
q2b = FMAT_query_bind(
  FMAT_query(
    "[MASK] is {TARGET}.",
    MASK = .(Male="He", Female="She"),
    TARGET = .(Occupation=cc("a doctor, a nurse, an artist"))
  ),
  FMAT_query(
    "[MASK] occupation is {TARGET}.",
    MASK = .(Male="His", Female="Her"),
    TARGET = .(Occupation=cc("doctor, nurse, artist"))
  )
)
d2b = FMAT_run(models, q2b)
summary(d2b, target.pair=FALSE)
```

```{r Ex2b Results, message=FALSE}
## Mean Score for Each Item
d2b.sum = summary(d2b, target.pair=FALSE)
d2b.sum$T_word = as_factor(str_remove(d2b.sum$T_word, "a |an "))
d2b.sum[, .(MASK = MASK[1], LPR_mean = mean(LPR)), keyby=T_word]
```

## Ex.3: `[MASK]` + `{TARGET}` + `{ATTRIB}`

This is the generic design of the FMAT. Here, when both `{TARGET}` and `{ATTRIB}` are specified, they will be forced to *listwise*.

### Ex.3a: `[MASK]` (pairwise) + `{TARGET}` (listwise) + `{ATTRIB}` (listwise)

> *Note*: From this example, we also see that even the *mean LPR score* for each target word (contrasted between two levels of attribute words; e.g., Pos vs. Neg) is ***insufficient*** for a proper interpretation of the association. The LPR scores should be further contrasted between two levels of target words. Therefore, the FMAT (and the WEAT) results are only interpretable and meaningful when the scores (log probability in FMAT, cosine similarity in WEAT) have been contrasted *both* between two attributes *and* between two targets. Single-target results without any contrasts may be misleading.

```{r Ex3a}
## FMAT
q3a = FMAT_query(
  "The {TARGET} has a [MASK] association with {ATTRIB}.",
  MASK = .(H="high", L="low"),
  TARGET = .(Flower=cc("rose, iris, lily"),
             Insect=cc("ant, cockroach, spider")),
  ATTRIB = .(Pos=cc("health, happiness, love, peace"),
             Neg=cc("death, sickness, hatred, disaster"))
)
d3a = FMAT_run(models, q3a)
summary(d3a)
```

```{r Ex3a Results, message=FALSE}
## Linear Mixed Model
## (more conservative and suitable for large number of random items)
lmm = lme(LPR ~ TARGET * model,
          random = ~ 1 | T_word,
          data = summary(d3a))
anova(lmm) %>% kable(digits=4)
emmeans(lmm, "TARGET") %>% contrast(method="pairwise") %>% kable(digits=4)
## For more usage of `emmip`, see help page: ?emmeans::emmip()
emmip(lmm, ~ TARGET | model, CIs=TRUE) +
  labs(x="Target", y="Log Probability Ratio (LPR)") +
  theme_bw(base_size=16)
```

### Ex.3b: `[MASK]` (pairwise) + `{TARGET}` (pairwise) + `{ATTRIB}` (pairwise)

Both `{TARGET}` and `{ATTRIB}` are still forced to *listwise* but are only designed in a *pairwise* way.

```{r Ex3b}
## FMAT
q3b = FMAT_query(
  "Most {TARGET} American people put [MASK] emphasis on {ATTRIB}.",
  MASK = .(H="high", L="low"),
  TARGET = .(Young=cc("young, younger"),
             Old=cc("old, older")),
  ATTRIB = .(IND=cc("individualism, being individualists"),
             COL=cc("collectivism, being collectivists"))
)
d3b = FMAT_run(models, q3b)
summary(d3b)
```

```{r Ex3b Results, message=FALSE}
## Linear Model
## contrasts: contr.sum
lm = lm(LPR ~ TARGET * model, data=summary(d3b))
car::Anova(lm, type=3) %>% kable()
emmeans(lm, "TARGET") %>% contrast(method="pairwise") %>% kable()
```

## Ex.4: `[MASK]` = years (for changes over time)

```{r Ex4}
## FMAT
q4 = FMAT_query(
  "Most American people valued {ATTRIB} in the year [MASK].",
  MASK = .(Year=1980:2000),
  ATTRIB = .(IND="individualism",
             COL="collectivism")
)
d4 = FMAT_run(models, q4)
```

```{r Ex4 Results, message=FALSE}
## Time Series Plot
d4.sum = summary(d4, mask.pair=FALSE)
d4.sum$Year = as.numeric(as.character(d4.sum$M_word))
d4.sum[, LPR_relative := LPR - LPR[1], by=model]  # ref = 1980

ggplot(d4.sum, aes(x=Year, y=LPR_relative, color=model)) +
  geom_path(alpha=0.8, linewidth=0.8) +
  labs(x="Year",
       y="Log Probability Ratio (LPR)\n(Reference Year = 1980)",
       color="Model",
       title="Fill-Mask Association Test",
       subtitle="Individualism (vs. Collectivism) of American People") +
  theme_bw(base_size=14)
```

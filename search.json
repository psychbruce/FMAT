[{"path":"https://psychbruce.github.io/FMAT/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Han-Wu-Shuang Bao. Author, maintainer.","code":""},{"path":"https://psychbruce.github.io/FMAT/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Bao H (2025). FMAT: Fill-Mask Association Test. R package version 2025.4, https://psychbruce.github.io/FMAT/.","code":"@Manual{,   title = {FMAT: The Fill-Mask Association Test},   author = {Han-Wu-Shuang Bao},   year = {2025},   note = {R package version 2025.4},   url = {https://psychbruce.github.io/FMAT/}, }"},{"path":"https://psychbruce.github.io/FMAT/index.html","id":"fmat-","dir":"","previous_headings":"","what":"The Fill-Mask Association Test","title":"The Fill-Mask Association Test","text":"üò∑ Fill-Mask Association Test (Êé©Á†ÅÂ°´Á©∫ËÅîÁ≥ªÊµãÈ™å). Fill-Mask Association Test (FMAT) integrative probability-based method using BERT Models measure conceptual associations (e.g., attitudes, biases, stereotypes, social norms, cultural values) propositions natural language (Bao, 2024, JPSP). ‚ö†Ô∏è Please update package version ‚â• 2025.4 faster robust functionality.","code":""},{"path":"https://psychbruce.github.io/FMAT/index.html","id":"author","dir":"","previous_headings":"","what":"Author","title":"The Fill-Mask Association Test","text":"Han-Wu-Shuang (Bruce) Bao ÂåÖÂØíÂê¥Èúú üì¨ baohws@foxmail.com üìã psychbruce.github.io","code":""},{"path":"https://psychbruce.github.io/FMAT/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"The Fill-Mask Association Test","text":"Note: original citation. Please refer information library(FMAT) APA-7 format version installed. Bao, H.-W.-S. (2024). Fill-Mask Association Test (FMAT): Measuring propositions natural language. Journal Personality Social Psychology, 127(3), 537‚Äì561. https://doi.org/10.1037/pspa0000396 Bao, H.-W.-S., & Gries, P. (2024). Intersectional race‚Äìgender stereotypes natural language. British Journal Social Psychology, 63(4), 1771‚Äì1786. https://doi.org/10.1111/bjso.12748","code":""},{"path":"https://psychbruce.github.io/FMAT/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"The Fill-Mask Association Test","text":"R package FMAT three Python packages (transformers, torch, huggingface-hub) need installed.","code":""},{"path":"https://psychbruce.github.io/FMAT/index.html","id":"id_1-r-package","dir":"","previous_headings":"Installation","what":"(1) R Package","title":"The Fill-Mask Association Test","text":"","code":"## Method 1: Install from CRAN install.packages(\"FMAT\")  ## Method 2: Install from GitHub install.packages(\"devtools\") devtools::install_github(\"psychbruce/FMAT\", force=TRUE)"},{"path":"https://psychbruce.github.io/FMAT/index.html","id":"id_2-python-environment-and-packages","dir":"","previous_headings":"Installation","what":"(2) Python Environment and Packages","title":"The Fill-Mask Association Test","text":"Install Anaconda (recommended package manager automatically installs Python, IDEs like Spyder, large list common Python packages). Specify Anaconda‚Äôs Python interpreter RStudio. RStudio ‚Üí Tools ‚Üí Global/Project Options ‚Üí Python ‚Üí Select ‚Üí Conda Environments ‚Üí Choose ‚Äú‚Ä¶/Anaconda3/python.exe‚Äù Install specific versions Python packages ‚Äútransformers‚Äù, ‚Äútorch‚Äù, ‚Äúhuggingface-hub‚Äù. (RStudio Terminal / Anaconda Prompt / Windows Command) CPU users: GPU (CUDA) users: use models (e.g., microsoft/deberta-v3-base), ‚Äúneed sentencepiece installed convert slow tokenizer fast one‚Äù: See Guidance GPU Acceleration installation guidance NVIDIA GPU device PC want use GPU accelerate pipeline. According May 2024 releases, ‚Äútransformers‚Äù ‚â• 4.41 depends ‚Äúhuggingface-hub‚Äù ‚â• 0.23. suggested versions ‚Äútransformers‚Äù (4.40.2) ‚Äúhuggingface-hub‚Äù (0.20.3) ensure console display progress bars downloading BERT models keeping packages new possible. Proxy users may use ‚Äúglobal mode‚Äù (ÂÖ®Â±ÄÊ®°Âºè) download models. https://www.cnblogs.com/xyz/p/17872452.html","code":"pip install transformers==4.40.2 torch==2.2.1 huggingface-hub==0.20.3 pip install transformers==4.40.2 huggingface-hub==0.20.3 pip install torch==2.2.1 --index-url https://download.pytorch.org/whl/cu121 pip install sentencepiece"},{"path":[]},{"path":"https://psychbruce.github.io/FMAT/index.html","id":"step-1-download-bert-models","dir":"","previous_headings":"Guidance for FMAT","what":"Step 1: Download BERT Models","title":"The Fill-Mask Association Test","text":"Use BERT_download() download BERT models. Model files saved local folder ‚Äú%USERPROFILE%/.cache/huggingface‚Äù. full list BERT models available Hugging Face. Use BERT_info() BERT_vocab() find detailed information BERT models.","code":""},{"path":"https://psychbruce.github.io/FMAT/index.html","id":"step-2-design-fmat-queries","dir":"","previous_headings":"Guidance for FMAT","what":"Step 2: Design FMAT Queries","title":"The Fill-Mask Association Test","text":"Design queries conceptually represent constructs measure (see Bao, 2024, JPSP design queries). Use FMAT_query() /FMAT_query_bind() prepare data.table queries.","code":""},{"path":"https://psychbruce.github.io/FMAT/index.html","id":"step-3-run-fmat","dir":"","previous_headings":"Guidance for FMAT","what":"Step 3: Run FMAT","title":"The Fill-Mask Association Test","text":"Use FMAT_run() get raw data (probability estimates) analysis. Several steps preprocessing included function easier use (see FMAT_run() details). BERT variants using <mask> rather [MASK] mask token, input query automatically modified users can always use [MASK] query design. BERT variants, special prefix characters \\u0120 \\u2581 automatically added match whole words (rather subwords) [MASK].","code":""},{"path":"https://psychbruce.github.io/FMAT/index.html","id":"notes","dir":"","previous_headings":"Guidance for FMAT","what":"Notes","title":"The Fill-Mask Association Test","text":"Improvements ongoing, especially adaptation diverse (less popular) BERT models. find bugs problems using functions, please report GitHub Issues send email.","code":""},{"path":"https://psychbruce.github.io/FMAT/index.html","id":"guidance-for-gpu-acceleration","dir":"","previous_headings":"","what":"Guidance for GPU Acceleration","title":"The Fill-Mask Association Test","text":"default, FMAT package uses CPU enable functionality users. advanced users want accelerate pipeline GPU, FMAT_run() function now supports using GPU device, 3x faster CPU. Test results (developer‚Äôs computer, depending BERT model size): CPU (Intel 13th-Gen i7-1355U): 500~1000 queries/min GPU (NVIDIA GeForce RTX 2050): 1500~3000 queries/min Checklist: Ensure NVIDIA GPU device (e.g., GeForce RTX Series) NVIDIA GPU driver installed system. Find guidance installation command https://pytorch.org/get-started/locally/. CUDA available Windows Linux, MacOS. installed version torch without CUDA support, please first uninstall (command: pip uninstall torch) install suggested one. may also install corresponding version CUDA Toolkit (e.g., torch version supporting CUDA 12.1, version CUDA Toolkit 12.1 may also installed). Example code installing PyTorch CUDA support: (RStudio Terminal / Anaconda Prompt / Windows Command)","code":"pip install torch==2.2.1 --index-url https://download.pytorch.org/whl/cu121"},{"path":"https://psychbruce.github.io/FMAT/index.html","id":"bert-models","dir":"","previous_headings":"","what":"BERT Models","title":"The Fill-Mask Association Test","text":"reliability validity following 12 BERT models FMAT established research, future work needed examine performance models. (model name Hugging Face - downloaded model file size) bert-base-uncased (420 MB) bert-base-cased (416 MB) bert-large-uncased (1283 MB) bert-large-cased (1277 MB) distilbert-base-uncased (256 MB) distilbert-base-cased (251 MB) albert-base-v1 (45 MB) albert-base-v2 (45 MB) roberta-base (476 MB) distilroberta-base (316 MB) vinai/bertweet-base (517 MB) vinai/bertweet-large (1356 MB) details BERT, see: Fill-Mask? [HuggingFace] Explorable BERT [HuggingFace] BERT Model Documentation [HuggingFace] Illustrated BERT Visual Guide BERT (Tested 2024-05-16 developer‚Äôs computer: HP Probook 450 G10 Notebook PC)","code":"library(FMAT) models = c(   \"bert-base-uncased\",   \"bert-base-cased\",   \"bert-large-uncased\",   \"bert-large-cased\",   \"distilbert-base-uncased\",   \"distilbert-base-cased\",   \"albert-base-v1\",   \"albert-base-v2\",   \"roberta-base\",   \"distilroberta-base\",   \"vinai/bertweet-base\",   \"vinai/bertweet-large\" ) BERT_download(models) ‚Ñπ Device Info:  R Packages: FMAT          2024.5 reticulate    1.36.1  Python Packages: transformers  4.40.2 torch         2.2.1+cu121  NVIDIA GPU CUDA Support: CUDA Enabled: TRUE CUDA Version: 12.1 GPU (Device): NVIDIA GeForce RTX 2050   ‚îÄ‚îÄ Downloading model \"bert-base-uncased\" ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚Üí (1) Downloading configuration... config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 570/570 [00:00<00:00, 114kB/s] ‚Üí (2) Downloading tokenizer... tokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48.0/48.0 [00:00<00:00, 23.9kB/s] vocab.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232k/232k [00:00<00:00, 1.50MB/s] tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 466k/466k [00:00<00:00, 1.98MB/s] ‚Üí (3) Downloading model... model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 440M/440M [00:36<00:00, 12.1MB/s] ‚úî Successfully downloaded model \"bert-base-uncased\"  ‚îÄ‚îÄ Downloading model \"bert-base-cased\" ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚Üí (1) Downloading configuration... config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 570/570 [00:00<00:00, 63.3kB/s] ‚Üí (2) Downloading tokenizer... tokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49.0/49.0 [00:00<00:00, 8.66kB/s] vocab.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213k/213k [00:00<00:00, 1.39MB/s] tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 436k/436k [00:00<00:00, 10.1MB/s] ‚Üí (3) Downloading model... model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 436M/436M [00:37<00:00, 11.6MB/s] ‚úî Successfully downloaded model \"bert-base-cased\"  ‚îÄ‚îÄ Downloading model \"bert-large-uncased\" ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚Üí (1) Downloading configuration... config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 571/571 [00:00<00:00, 268kB/s] ‚Üí (2) Downloading tokenizer... tokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48.0/48.0 [00:00<00:00, 12.0kB/s] vocab.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232k/232k [00:00<00:00, 1.50MB/s] tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 466k/466k [00:00<00:00, 1.99MB/s] ‚Üí (3) Downloading model... model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.34G/1.34G [01:36<00:00, 14.0MB/s] ‚úî Successfully downloaded model \"bert-large-uncased\"  ‚îÄ‚îÄ Downloading model \"bert-large-cased\" ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚Üí (1) Downloading configuration... config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 762/762 [00:00<00:00, 125kB/s] ‚Üí (2) Downloading tokenizer... tokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49.0/49.0 [00:00<00:00, 12.3kB/s] vocab.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213k/213k [00:00<00:00, 1.41MB/s] tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 436k/436k [00:00<00:00, 5.39MB/s] ‚Üí (3) Downloading model... model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.34G/1.34G [01:35<00:00, 14.0MB/s] ‚úî Successfully downloaded model \"bert-large-cased\"  ‚îÄ‚îÄ Downloading model \"distilbert-base-uncased\" ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚Üí (1) Downloading configuration... config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 483/483 [00:00<00:00, 161kB/s] ‚Üí (2) Downloading tokenizer... tokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48.0/48.0 [00:00<00:00, 9.46kB/s] vocab.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232k/232k [00:00<00:00, 16.5MB/s] tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 466k/466k [00:00<00:00, 14.8MB/s] ‚Üí (3) Downloading model... model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 268M/268M [00:19<00:00, 13.5MB/s] ‚úî Successfully downloaded model \"distilbert-base-uncased\"  ‚îÄ‚îÄ Downloading model \"distilbert-base-cased\" ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚Üí (1) Downloading configuration... config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 465/465 [00:00<00:00, 233kB/s] ‚Üí (2) Downloading tokenizer... tokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49.0/49.0 [00:00<00:00, 9.80kB/s] vocab.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213k/213k [00:00<00:00, 1.39MB/s] tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 436k/436k [00:00<00:00, 8.70MB/s] ‚Üí (3) Downloading model... model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 263M/263M [00:24<00:00, 10.9MB/s] ‚úî Successfully downloaded model \"distilbert-base-cased\"  ‚îÄ‚îÄ Downloading model \"albert-base-v1\" ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚Üí (1) Downloading configuration... config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 684/684 [00:00<00:00, 137kB/s] ‚Üí (2) Downloading tokenizer... tokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25.0/25.0 [00:00<00:00, 3.57kB/s] spiece.model: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 760k/760k [00:00<00:00, 4.93MB/s] tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.31M/1.31M [00:00<00:00, 13.4MB/s] ‚Üí (3) Downloading model... model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47.4M/47.4M [00:03<00:00, 13.4MB/s] ‚úî Successfully downloaded model \"albert-base-v1\"  ‚îÄ‚îÄ Downloading model \"albert-base-v2\" ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚Üí (1) Downloading configuration... config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 684/684 [00:00<00:00, 137kB/s] ‚Üí (2) Downloading tokenizer... tokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25.0/25.0 [00:00<00:00, 4.17kB/s] spiece.model: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 760k/760k [00:00<00:00, 5.10MB/s] tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.31M/1.31M [00:00<00:00, 6.93MB/s] ‚Üí (3) Downloading model... model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47.4M/47.4M [00:03<00:00, 13.8MB/s] ‚úî Successfully downloaded model \"albert-base-v2\"  ‚îÄ‚îÄ Downloading model \"roberta-base\" ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚Üí (1) Downloading configuration... config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 481/481 [00:00<00:00, 80.3kB/s] ‚Üí (2) Downloading tokenizer... tokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25.0/25.0 [00:00<00:00, 6.25kB/s] vocab.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 899k/899k [00:00<00:00, 2.72MB/s] merges.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 456k/456k [00:00<00:00, 8.22MB/s] tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.36M/1.36M [00:00<00:00, 8.56MB/s] ‚Üí (3) Downloading model... model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 499M/499M [00:38<00:00, 12.9MB/s] ‚úî Successfully downloaded model \"roberta-base\"  ‚îÄ‚îÄ Downloading model \"distilroberta-base\" ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚Üí (1) Downloading configuration... config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 480/480 [00:00<00:00, 96.4kB/s] ‚Üí (2) Downloading tokenizer... tokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25.0/25.0 [00:00<00:00, 12.0kB/s] vocab.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 899k/899k [00:00<00:00, 6.59MB/s] merges.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 456k/456k [00:00<00:00, 9.46MB/s] tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.36M/1.36M [00:00<00:00, 11.5MB/s] ‚Üí (3) Downloading model... model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 331M/331M [00:25<00:00, 13.0MB/s] ‚úî Successfully downloaded model \"distilroberta-base\"  ‚îÄ‚îÄ Downloading model \"vinai/bertweet-base\" ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚Üí (1) Downloading configuration... config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 558/558 [00:00<00:00, 187kB/s] ‚Üí (2) Downloading tokenizer... vocab.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 843k/843k [00:00<00:00, 7.44MB/s] bpe.codes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.08M/1.08M [00:00<00:00, 7.01MB/s] tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.91M/2.91M [00:00<00:00, 9.10MB/s] ‚Üí (3) Downloading model... pytorch_model.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 543M/543M [00:48<00:00, 11.1MB/s] ‚úî Successfully downloaded model \"vinai/bertweet-base\"  ‚îÄ‚îÄ Downloading model \"vinai/bertweet-large\" ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚Üí (1) Downloading configuration... config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 614/614 [00:00<00:00, 120kB/s] ‚Üí (2) Downloading tokenizer... vocab.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 899k/899k [00:00<00:00, 5.90MB/s] merges.txt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 456k/456k [00:00<00:00, 7.30MB/s] tokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.36M/1.36M [00:00<00:00, 8.31MB/s] ‚Üí (3) Downloading model... pytorch_model.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.42G/1.42G [02:29<00:00, 9.53MB/s] ‚úî Successfully downloaded model \"vinai/bertweet-large\"  ‚îÄ‚îÄ Downloaded models: ‚îÄ‚îÄ                             size albert-base-v1            45 MB albert-base-v2            45 MB bert-base-cased          416 MB bert-base-uncased        420 MB bert-large-cased        1277 MB bert-large-uncased      1283 MB distilbert-base-cased    251 MB distilbert-base-uncased  256 MB distilroberta-base       316 MB roberta-base             476 MB vinai/bertweet-base      517 MB vinai/bertweet-large    1356 MB  ‚úî Downloaded models saved at C:/Users/Bruce/.cache/huggingface/hub (6.52 GB) BERT_info(models) model   size vocab  dims   mask                      <fctr> <char> <int> <int> <char>  1:       bert-base-uncased  420MB 30522   768 [MASK]  2:         bert-base-cased  416MB 28996   768 [MASK]  3:      bert-large-uncased 1283MB 30522  1024 [MASK]  4:        bert-large-cased 1277MB 28996  1024 [MASK]  5: distilbert-base-uncased  256MB 30522   768 [MASK]  6:   distilbert-base-cased  251MB 28996   768 [MASK]  7:          albert-base-v1   45MB 30000   128 [MASK]  8:          albert-base-v2   45MB 30000   128 [MASK]  9:            roberta-base  476MB 50265   768 <mask> 10:      distilroberta-base  316MB 50265   768 <mask> 11:     vinai/bertweet-base  517MB 64001   768 <mask> 12:    vinai/bertweet-large 1356MB 50265  1024 <mask>"},{"path":"https://psychbruce.github.io/FMAT/index.html","id":"related-packages","dir":"","previous_headings":"","what":"Related Packages","title":"The Fill-Mask Association Test","text":"FMAT innovative method computational intelligent analysis psychology society, may also seek integrative toolbox text-analytic methods. Another R package developed‚ÄîPsychWordVec‚Äîuseful user-friendly word embedding analysis (e.g., Word Embedding Association Test, WEAT). Please refer documentation feel free use .","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/BERT_download.html","id":null,"dir":"Reference","previous_headings":"","what":"Download and save BERT models to local cache folder. ‚Äî BERT_download","title":"Download and save BERT models to local cache folder. ‚Äî BERT_download","text":"Download save BERT models local cache folder \"%USERPROFILE%/.cache/huggingface\".","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/BERT_download.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download and save BERT models to local cache folder. ‚Äî BERT_download","text":"","code":"BERT_download(models = NULL, verbose = FALSE)"},{"path":"https://psychbruce.github.io/FMAT/reference/BERT_download.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download and save BERT models to local cache folder. ‚Äî BERT_download","text":"models Model names HuggingFace. verbose Alert model downloaded. Defaults FALSE.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/BERT_download.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download and save BERT models to local cache folder. ‚Äî BERT_download","text":"Invisibly return data.table basic file information local models.","code":""},{"path":[]},{"path":"https://psychbruce.github.io/FMAT/reference/BERT_download.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download and save BERT models to local cache folder. ‚Äî BERT_download","text":"","code":"if (FALSE) { # \\dontrun{ models = c(\"bert-base-uncased\", \"bert-base-cased\") BERT_download(models)  BERT_download()  # check downloaded models  BERT_info()  # information of all downloaded models } # }"},{"path":"https://psychbruce.github.io/FMAT/reference/BERT_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Get basic information of BERT models. ‚Äî BERT_info","title":"Get basic information of BERT models. ‚Äî BERT_info","text":"Get basic information BERT models.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/BERT_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get basic information of BERT models. ‚Äî BERT_info","text":"","code":"BERT_info(models = NULL)"},{"path":"https://psychbruce.github.io/FMAT/reference/BERT_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get basic information of BERT models. ‚Äî BERT_info","text":"models Model names HuggingFace.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/BERT_info.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get basic information of BERT models. ‚Äî BERT_info","text":"data.table: model name model type number parameters vocabulary size (input token embeddings) embedding dimensions (input token embeddings) hidden layers attention heads [MASK] token","code":""},{"path":[]},{"path":"https://psychbruce.github.io/FMAT/reference/BERT_info.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get basic information of BERT models. ‚Äî BERT_info","text":"","code":"if (FALSE) { # \\dontrun{ models = c(\"bert-base-uncased\", \"bert-base-cased\") BERT_info(models)  BERT_info()  # information of all downloaded models # speed: ~1.2s/model for first use; <1s afterwards } # }"},{"path":"https://psychbruce.github.io/FMAT/reference/BERT_info_date.html","id":null,"dir":"Reference","previous_headings":"","what":"Scrape the initial commit date of BERT models. ‚Äî BERT_info_date","title":"Scrape the initial commit date of BERT models. ‚Äî BERT_info_date","text":"Scrape initial commit date BERT models.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/BERT_info_date.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scrape the initial commit date of BERT models. ‚Äî BERT_info_date","text":"","code":"BERT_info_date(models = NULL)"},{"path":"https://psychbruce.github.io/FMAT/reference/BERT_info_date.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scrape the initial commit date of BERT models. ‚Äî BERT_info_date","text":"models Model names HuggingFace.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/BERT_info_date.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Scrape the initial commit date of BERT models. ‚Äî BERT_info_date","text":"data.table: model name initial commit date (scraped huggingface commit history)","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/BERT_info_date.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scrape the initial commit date of BERT models. ‚Äî BERT_info_date","text":"","code":"if (FALSE) { # \\dontrun{ model.date = BERT_info_date() # get all models from cache folder  one.model.date = FMAT:::get_model_date(\"bert-base-uncased\") # call the internal function to scrape a model # that may not have been saved in cache folder } # }"},{"path":"https://psychbruce.github.io/FMAT/reference/BERT_vocab.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if mask words are in the model vocabulary. ‚Äî BERT_vocab","title":"Check if mask words are in the model vocabulary. ‚Äî BERT_vocab","text":"Check mask words model vocabulary.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/BERT_vocab.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if mask words are in the model vocabulary. ‚Äî BERT_vocab","text":"","code":"BERT_vocab(   models,   mask.words,   add.tokens = FALSE,   add.method = c(\"sum\", \"mean\") )"},{"path":"https://psychbruce.github.io/FMAT/reference/BERT_vocab.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if mask words are in the model vocabulary. ‚Äî BERT_vocab","text":"models Model names HuggingFace. mask.words Option words filling mask. add.tokens Add new tokens (--vocabulary words even phrases) model vocabulary? Defaults FALSE. temporarily adds tokens tasks change raw model file. add.method Method used produce token embeddings new added tokens. Can \"sum\" (default) \"mean\" subword token embeddings.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/BERT_vocab.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if mask words are in the model vocabulary. ‚Äî BERT_vocab","text":"data.table model name, mask word, real token (replaced vocabulary), token id (0~N).","code":""},{"path":[]},{"path":"https://psychbruce.github.io/FMAT/reference/BERT_vocab.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check if mask words are in the model vocabulary. ‚Äî BERT_vocab","text":"","code":"if (FALSE) { # \\dontrun{ models = c(\"bert-base-uncased\", \"bert-base-cased\") BERT_info(models)  BERT_vocab(models, c(\"bruce\", \"Bruce\"))  BERT_vocab(models, 2020:2025)  # some are out-of-vocabulary BERT_vocab(models, 2020:2025, add.tokens=TRUE)  # add vocab  BERT_vocab(models,            c(\"individualism\", \"artificial intelligence\"),            add.tokens=TRUE) } # }"},{"path":"https://psychbruce.github.io/FMAT/reference/FMAT_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare a data.table of queries and variables for the FMAT. ‚Äî FMAT_query","title":"Prepare a data.table of queries and variables for the FMAT. ‚Äî FMAT_query","text":"Prepare data.table queries variables FMAT.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/FMAT_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare a data.table of queries and variables for the FMAT. ‚Äî FMAT_query","text":"","code":"FMAT_query(   query = \"Text with [MASK], optionally with {TARGET} and/or {ATTRIB}.\",   MASK = .(),   TARGET = .(),   ATTRIB = .() )"},{"path":"https://psychbruce.github.io/FMAT/reference/FMAT_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare a data.table of queries and variables for the FMAT. ‚Äî FMAT_query","text":"query Query text (character string/vector least one [MASK] token). Multiple queries share set MASK, TARGET, ATTRIB. multiple queries different MASK, TARGET, /ATTRIB, please use FMAT_query_bind combine . MASK named list [MASK] target words. Must single words vocabulary certain masked language model. model vocabulary, see, e.g., https://huggingface.co/bert-base-uncased/raw/main/vocab.txt Infrequent words may included model's vocabulary, case may insert words context specifying either TARGET ATTRIB. TARGET, ATTRIB named list Target/Attribute words phrases. specified, query must contain {TARGET} /{ATTRIB} (uppercase braces) replaced words/phrases.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/FMAT_query.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Prepare a data.table of queries and variables for the FMAT. ‚Äî FMAT_query","text":"data.table queries variables.","code":""},{"path":[]},{"path":"https://psychbruce.github.io/FMAT/reference/FMAT_query.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare a data.table of queries and variables for the FMAT. ‚Äî FMAT_query","text":"","code":"FMAT_query(\"[MASK] is a nurse.\", MASK = .(Male=\"He\", Female=\"She\")) #>                 query   MASK M_pair M_word #>                <fctr> <fctr> <fctr> <fctr> #> 1: [MASK] is a nurse.   Male      1     He #> 2: [MASK] is a nurse. Female      1    She  FMAT_query(   c(\"[MASK] is {TARGET}.\", \"[MASK] works as {TARGET}.\"),   MASK = .(Male=\"He\", Female=\"She\"),   TARGET = .(Occupation=c(\"a doctor\", \"a nurse\", \"an artist\")) ) #>        qid                     query   MASK M_pair M_word     TARGET #>     <fctr>                    <fctr> <fctr> <fctr> <fctr>     <fctr> #>  1:      1       [MASK] is {TARGET}.   Male      1     He Occupation #>  2:      1       [MASK] is {TARGET}. Female      1    She Occupation #>  3:      1       [MASK] is {TARGET}.   Male      1     He Occupation #>  4:      1       [MASK] is {TARGET}. Female      1    She Occupation #>  5:      1       [MASK] is {TARGET}.   Male      1     He Occupation #>  6:      1       [MASK] is {TARGET}. Female      1    She Occupation #>  7:      2 [MASK] works as {TARGET}.   Male      1     He Occupation #>  8:      2 [MASK] works as {TARGET}. Female      1    She Occupation #>  9:      2 [MASK] works as {TARGET}.   Male      1     He Occupation #> 10:      2 [MASK] works as {TARGET}. Female      1    She Occupation #> 11:      2 [MASK] works as {TARGET}.   Male      1     He Occupation #> 12:      2 [MASK] works as {TARGET}. Female      1    She Occupation #>           T_pair    T_word #>           <fctr>    <fctr> #>  1: Occupation.1  a doctor #>  2: Occupation.1  a doctor #>  3: Occupation.2   a nurse #>  4: Occupation.2   a nurse #>  5: Occupation.3 an artist #>  6: Occupation.3 an artist #>  7: Occupation.1  a doctor #>  8: Occupation.1  a doctor #>  9: Occupation.2   a nurse #> 10: Occupation.2   a nurse #> 11: Occupation.3 an artist #> 12: Occupation.3 an artist  FMAT_query(   \"The [MASK] {ATTRIB}.\",   MASK = .(Male=c(\"man\", \"boy\"),            Female=c(\"woman\", \"girl\")),   ATTRIB = .(Masc=c(\"is masculine\", \"has a masculine personality\"),              Femi=c(\"is feminine\", \"has a feminine personality\")) ) #>                    query   MASK M_pair M_word ATTRIB      A_pair #>                   <fctr> <fctr> <fctr> <fctr> <fctr>      <fctr> #>  1: The [MASK] {ATTRIB}.   Male      1    man   Masc Masc-Femi.1 #>  2: The [MASK] {ATTRIB}.   Male      2    boy   Masc Masc-Femi.1 #>  3: The [MASK] {ATTRIB}. Female      1  woman   Masc Masc-Femi.1 #>  4: The [MASK] {ATTRIB}. Female      2   girl   Masc Masc-Femi.1 #>  5: The [MASK] {ATTRIB}.   Male      1    man   Masc Masc-Femi.2 #>  6: The [MASK] {ATTRIB}.   Male      2    boy   Masc Masc-Femi.2 #>  7: The [MASK] {ATTRIB}. Female      1  woman   Masc Masc-Femi.2 #>  8: The [MASK] {ATTRIB}. Female      2   girl   Masc Masc-Femi.2 #>  9: The [MASK] {ATTRIB}.   Male      1    man   Femi Masc-Femi.1 #> 10: The [MASK] {ATTRIB}.   Male      2    boy   Femi Masc-Femi.1 #> 11: The [MASK] {ATTRIB}. Female      1  woman   Femi Masc-Femi.1 #> 12: The [MASK] {ATTRIB}. Female      2   girl   Femi Masc-Femi.1 #> 13: The [MASK] {ATTRIB}.   Male      1    man   Femi Masc-Femi.2 #> 14: The [MASK] {ATTRIB}.   Male      2    boy   Femi Masc-Femi.2 #> 15: The [MASK] {ATTRIB}. Female      1  woman   Femi Masc-Femi.2 #> 16: The [MASK] {ATTRIB}. Female      2   girl   Femi Masc-Femi.2 #>                          A_word #>                          <fctr> #>  1:                is masculine #>  2:                is masculine #>  3:                is masculine #>  4:                is masculine #>  5: has a masculine personality #>  6: has a masculine personality #>  7: has a masculine personality #>  8: has a masculine personality #>  9:                 is feminine #> 10:                 is feminine #> 11:                 is feminine #> 12:                 is feminine #> 13:  has a feminine personality #> 14:  has a feminine personality #> 15:  has a feminine personality #> 16:  has a feminine personality"},{"path":"https://psychbruce.github.io/FMAT/reference/FMAT_query_bind.html","id":null,"dir":"Reference","previous_headings":"","what":"Combine multiple query data.tables and renumber query ids. ‚Äî FMAT_query_bind","title":"Combine multiple query data.tables and renumber query ids. ‚Äî FMAT_query_bind","text":"Combine multiple query data.tables renumber query ids.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/FMAT_query_bind.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combine multiple query data.tables and renumber query ids. ‚Äî FMAT_query_bind","text":"","code":"FMAT_query_bind(...)"},{"path":"https://psychbruce.github.io/FMAT/reference/FMAT_query_bind.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combine multiple query data.tables and renumber query ids. ‚Äî FMAT_query_bind","text":"... Query data.tables returned FMAT_query.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/FMAT_query_bind.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Combine multiple query data.tables and renumber query ids. ‚Äî FMAT_query_bind","text":"data.table queries variables.","code":""},{"path":[]},{"path":"https://psychbruce.github.io/FMAT/reference/FMAT_query_bind.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Combine multiple query data.tables and renumber query ids. ‚Äî FMAT_query_bind","text":"","code":"FMAT_query_bind(   FMAT_query(     \"[MASK] is {TARGET}.\",     MASK = .(Male=\"He\", Female=\"She\"),     TARGET = .(Occupation=c(\"a doctor\", \"a nurse\", \"an artist\"))   ),   FMAT_query(     \"[MASK] occupation is {TARGET}.\",     MASK = .(Male=\"His\", Female=\"Her\"),     TARGET = .(Occupation=c(\"doctor\", \"nurse\", \"artist\"))   ) ) #>        qid                          query   MASK M_pair M_word     TARGET #>     <fctr>                         <fctr> <fctr> <fctr> <fctr>     <fctr> #>  1:      1            [MASK] is {TARGET}.   Male      1     He Occupation #>  2:      1            [MASK] is {TARGET}. Female      1    She Occupation #>  3:      1            [MASK] is {TARGET}.   Male      1     He Occupation #>  4:      1            [MASK] is {TARGET}. Female      1    She Occupation #>  5:      1            [MASK] is {TARGET}.   Male      1     He Occupation #>  6:      1            [MASK] is {TARGET}. Female      1    She Occupation #>  7:      2 [MASK] occupation is {TARGET}.   Male      1    His Occupation #>  8:      2 [MASK] occupation is {TARGET}. Female      1    Her Occupation #>  9:      2 [MASK] occupation is {TARGET}.   Male      1    His Occupation #> 10:      2 [MASK] occupation is {TARGET}. Female      1    Her Occupation #> 11:      2 [MASK] occupation is {TARGET}.   Male      1    His Occupation #> 12:      2 [MASK] occupation is {TARGET}. Female      1    Her Occupation #>           T_pair    T_word #>           <fctr>    <fctr> #>  1: Occupation.1  a doctor #>  2: Occupation.1  a doctor #>  3: Occupation.2   a nurse #>  4: Occupation.2   a nurse #>  5: Occupation.3 an artist #>  6: Occupation.3 an artist #>  7: Occupation.1    doctor #>  8: Occupation.1    doctor #>  9: Occupation.2     nurse #> 10: Occupation.2     nurse #> 11: Occupation.3    artist #> 12: Occupation.3    artist"},{"path":"https://psychbruce.github.io/FMAT/reference/FMAT_run.html","id":null,"dir":"Reference","previous_headings":"","what":"Run the fill-mask pipeline on multiple models (CPU / GPU). ‚Äî FMAT_run","title":"Run the fill-mask pipeline on multiple models (CPU / GPU). ‚Äî FMAT_run","text":"Run fill-mask pipeline multiple models CPU GPU (faster requiring NVIDIA GPU device).","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/FMAT_run.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run the fill-mask pipeline on multiple models (CPU / GPU). ‚Äî FMAT_run","text":"","code":"FMAT_run(   models,   data,   gpu,   add.tokens = FALSE,   add.method = c(\"sum\", \"mean\"),   pattern.special = list(uncased = \"uncased|albert|electra|muhtasham\", prefix.u2581 =     \"albert|xlm-roberta|xlnet\", prefix.u2581.excl = \"chinese\", prefix.u0120 =     \"roberta|bart|deberta|bertweet-large\", prefix.u0120.excl = \"chinese|xlm-|kornosk/\"),   file = NULL,   progress = TRUE,   warning = TRUE,   na.out = TRUE )"},{"path":"https://psychbruce.github.io/FMAT/reference/FMAT_run.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run the fill-mask pipeline on multiple models (CPU / GPU). ‚Äî FMAT_run","text":"models Options: character vector model names HuggingFace. Can used CPU GPU. returned object FMAT_load. Can used CPU. restart R session, need rerun FMAT_load. data data.table returned FMAT_query FMAT_query_bind. gpu Use GPU (3x faster CPU) run fill-mask pipeline? Defaults missing value automatically use available GPU (available, use CPU). NVIDIA GPU device (e.g., GeForce RTX Series) required use GPU. See Guidance GPU Acceleration. Options passing device parameter Python: FALSE: CPU (device = -1). TRUE: GPU (device = 0). value: passing transformers.pipeline(device=...) defines device (e.g., \"cpu\", \"cuda:0\", GPU device id like 1) pipeline allocated. add.tokens Add new tokens (--vocabulary words even phrases) model vocabulary? Defaults FALSE. temporarily adds tokens tasks change raw model file. add.method Method used produce token embeddings new added tokens. Can \"sum\" (default) \"mean\" subword token embeddings. pattern.special Regular expression patterns (matching model names) special model cases uncased require special prefix character certain situations. WARNING: developer able check models, users responsible checking models use modifying argument necessary. prefix.u2581: adding prefix \\u2581 mask words prefix.u0120: adding prefix \\u0120 non-starting mask words file File name .RData save returned data. progress Show progress bar? Defaults TRUE. warning Alert warning --vocabulary word(s)? Defaults TRUE. na.Replace probabilities --vocabulary word(s) NA? Defaults TRUE.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/FMAT_run.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run the fill-mask pipeline on multiple models (CPU / GPU). ‚Äî FMAT_run","text":"data.table (new class fmat) appending data new variables: model: model name. output: complete sentence output unmasked token. token: actual token filled blank mask (note \"--vocabulary\" added original word found model vocabulary). prob: (raw) conditional probability unmasked token given provided context, estimated masked language model. SUGGESTED directly interpret raw probabilities contrast pair probabilities interpretable. See summary.fmat.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/FMAT_run.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Run the fill-mask pipeline on multiple models (CPU / GPU). ‚Äî FMAT_run","text":"function automatically adjusts compatibility tokens used certain models: (1) uncased models (e.g., ALBERT), turns tokens lowercase; (2) models use <mask> rather [MASK], automatically uses corrected mask token; (3) models require prefix estimate whole words subwords (e.g., ALBERT, RoBERTa), adds certain prefix (usually white space; \\u2581 ALBERT XLM-RoBERTa, \\u0120 RoBERTa DistilRoBERTa). Note changes affect token variable returned data, affect M_word variable. Thus, users may analyze data based unchanged M_word rather token. Note also may extremely trivial differences (5~6 significant digits) raw probability estimates using CPU GPU, differences little impact main results.","code":""},{"path":[]},{"path":"https://psychbruce.github.io/FMAT/reference/FMAT_run.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run the fill-mask pipeline on multiple models (CPU / GPU). ‚Äî FMAT_run","text":"","code":"## Running the examples requires the models downloaded  if (FALSE) { # \\dontrun{ models = c(\"bert-base-uncased\", \"bert-base-cased\")  query1 = FMAT_query(   c(\"[MASK] is {TARGET}.\", \"[MASK] works as {TARGET}.\"),   MASK = .(Male=\"He\", Female=\"She\"),   TARGET = .(Occupation=c(\"a doctor\", \"a nurse\", \"an artist\")) ) data1 = FMAT_run(models, query1) summary(data1, target.pair=FALSE)  query2 = FMAT_query(   \"The [MASK] {ATTRIB}.\",   MASK = .(Male=c(\"man\", \"boy\"),            Female=c(\"woman\", \"girl\")),   ATTRIB = .(Masc=c(\"is masculine\", \"has a masculine personality\"),              Femi=c(\"is feminine\", \"has a feminine personality\")) ) data2 = FMAT_run(models, query2) summary(data2, mask.pair=FALSE) summary(data2) } # }"},{"path":"https://psychbruce.github.io/FMAT/reference/ICC_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Intraclass correlation coefficient (ICC) of BERT models. ‚Äî ICC_models","title":"Intraclass correlation coefficient (ICC) of BERT models. ‚Äî ICC_models","text":"Interrater agreement log probabilities (treated \"ratings\"/rows) among BERT language models (treated \"raters\"/columns), row column (\"two-way\") random effects.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/ICC_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Intraclass correlation coefficient (ICC) of BERT models. ‚Äî ICC_models","text":"","code":"ICC_models(data, type = \"agreement\", unit = \"average\")"},{"path":"https://psychbruce.github.io/FMAT/reference/ICC_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Intraclass correlation coefficient (ICC) of BERT models. ‚Äî ICC_models","text":"data Raw data returned FMAT_run. type Interrater \"agreement\" (default) \"consistency\". unit Reliability \"average\" scores (default) \"single\" scores.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/ICC_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Intraclass correlation coefficient (ICC) of BERT models. ‚Äî ICC_models","text":"data.table ICC.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/LPR_reliability.html","id":null,"dir":"Reference","previous_headings":"","what":"Reliability analysis (Cronbach's \\(\\alpha\\)) of LPR. ‚Äî LPR_reliability","title":"Reliability analysis (Cronbach's \\(\\alpha\\)) of LPR. ‚Äî LPR_reliability","text":"Reliability analysis (Cronbach's \\(\\alpha\\)) LPR.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/LPR_reliability.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reliability analysis (Cronbach's \\(\\alpha\\)) of LPR. ‚Äî LPR_reliability","text":"","code":"LPR_reliability(fmat, item = c(\"query\", \"T_word\", \"A_word\"), by = NULL)"},{"path":"https://psychbruce.github.io/FMAT/reference/LPR_reliability.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reliability analysis (Cronbach's \\(\\alpha\\)) of LPR. ‚Äî LPR_reliability","text":"fmat data.table returned summary.fmat. item Reliability multiple \"query\" (default), \"T_word\", \"A_word\". Variable(s) split data . Options can \"model\", \"TARGET\", \"ATTRIB\", combination .","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/LPR_reliability.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reliability analysis (Cronbach's \\(\\alpha\\)) of LPR. ‚Äî LPR_reliability","text":"data.table Cronbach's \\(\\alpha\\).","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/dot-.html","id":null,"dir":"Reference","previous_headings":"","what":"A simple function equivalent to list. ‚Äî .","title":"A simple function equivalent to list. ‚Äî .","text":"simple function equivalent list.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/dot-.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A simple function equivalent to list. ‚Äî .","text":"","code":".(...)"},{"path":"https://psychbruce.github.io/FMAT/reference/dot-.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"A simple function equivalent to list. ‚Äî .","text":"... Named objects (usually character vectors package).","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/dot-.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"A simple function equivalent to list. ‚Äî .","text":"list named objects.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/dot-.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A simple function equivalent to list. ‚Äî .","text":"","code":".(Male=c(\"he\", \"his\"), Female=c(\"she\", \"her\")) #> $Male #> [1] \"he\"  \"his\" #>  #> $Female #> [1] \"she\" \"her\" #>"},{"path":"https://psychbruce.github.io/FMAT/reference/fill_mask.html","id":null,"dir":"Reference","previous_headings":"","what":"Run the fill-mask pipeline and check the raw results. ‚Äî fill_mask","title":"Run the fill-mask pipeline and check the raw results. ‚Äî fill_mask","text":"Normal users use FMAT_run(). function technical check.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/fill_mask.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run the fill-mask pipeline and check the raw results. ‚Äî fill_mask","text":"","code":"fill_mask(query, model, targets = NULL, topn = 5, gpu)  fill_mask_check(query, models, targets = NULL, topn = 5, gpu)"},{"path":"https://psychbruce.github.io/FMAT/reference/fill_mask.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run the fill-mask pipeline and check the raw results. ‚Äî fill_mask","text":"query Query sentence mask token. model, models Model name(s). targets Target words fill mask. Defaults NULL (return top 5 likely words). topn Number likely predictions return. Defaults 5. gpu Use GPU (3x faster CPU) run fill-mask pipeline? Defaults missing value automatically use available GPU (available, use CPU). NVIDIA GPU device (e.g., GeForce RTX Series) required use GPU. See Guidance GPU Acceleration. Options passing device parameter Python: FALSE: CPU (device = -1). TRUE: GPU (device = 0). value: passing transformers.pipeline(device=...) defines device (e.g., \"cpu\", \"cuda:0\", GPU device id like 1) pipeline allocated.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/fill_mask.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run the fill-mask pipeline and check the raw results. ‚Äî fill_mask","text":"data.table raw results.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/fill_mask.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Run the fill-mask pipeline and check the raw results. ‚Äî fill_mask","text":"fill_mask(): Check performance one model. fill_mask_check(): Check performance multiple models.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/fill_mask.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run the fill-mask pipeline and check the raw results. ‚Äî fill_mask","text":"","code":"if (FALSE) { # \\dontrun{ query = \"Paris is the [MASK] of France.\" models = c(\"bert-base-uncased\", \"bert-base-cased\")  d.check = fill_mask_check(query, models, topn=2) } # }"},{"path":"https://psychbruce.github.io/FMAT/reference/set_cache_folder.html","id":null,"dir":"Reference","previous_headings":"","what":"Set (change) HuggingFace cache folder temporarily. ‚Äî set_cache_folder","title":"Set (change) HuggingFace cache folder temporarily. ‚Äî set_cache_folder","text":"function allows change default cache directory (lacks disk capacity) another path (e.g., portable SSD) temporarily. Keep mind: function takes effect current R session temporarily, run time use FMAT functions R session.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/set_cache_folder.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set (change) HuggingFace cache folder temporarily. ‚Äî set_cache_folder","text":"","code":"set_cache_folder(path)"},{"path":"https://psychbruce.github.io/FMAT/reference/set_cache_folder.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set (change) HuggingFace cache folder temporarily. ‚Äî set_cache_folder","text":"path Folder path store HuggingFace models.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/set_cache_folder.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set (change) HuggingFace cache folder temporarily. ‚Äî set_cache_folder","text":"","code":"if (FALSE) { # \\dontrun{ library(FMAT) set_cache_folder(\"D:/huggingface_cache/\") # -> models would be saved to \"D:/huggingface_cache/hub/\" # run this function each time before using FMAT functions  BERT_download() BERT_info() } # }"},{"path":"https://psychbruce.github.io/FMAT/reference/summary.fmat.html","id":null,"dir":"Reference","previous_headings":"","what":"[S3 method] Summarize the results for the FMAT. ‚Äî summary.fmat","title":"[S3 method] Summarize the results for the FMAT. ‚Äî summary.fmat","text":"Summarize results Log Probability Ratio (LPR), indicates relative (vs. absolute) association concepts. LPR just one contrast (e.g., pair attributes) may sufficient proper interpretation results, may require second contrast (e.g., pair targets). Users suggested use linear mixed models (R packages nlme lme4/lmerTest) perform formal analyses hypothesis tests based LPR.","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/summary.fmat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"[S3 method] Summarize the results for the FMAT. ‚Äî summary.fmat","text":"","code":"# S3 method for class 'fmat' summary(   object,   mask.pair = TRUE,   target.pair = TRUE,   attrib.pair = TRUE,   warning = TRUE,   ... )"},{"path":"https://psychbruce.github.io/FMAT/reference/summary.fmat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"[S3 method] Summarize the results for the FMAT. ‚Äî summary.fmat","text":"object data.table (new class fmat) returned FMAT_run. mask.pair, target.pair, attrib.pair Pairwise contrast [MASK], TARGET, ATTRIB? Defaults TRUE. warning Alert warning --vocabulary word(s)? Defaults TRUE. ... arguments (currently used).","code":""},{"path":"https://psychbruce.github.io/FMAT/reference/summary.fmat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"[S3 method] Summarize the results for the FMAT. ‚Äî summary.fmat","text":"data.table summarized results Log Probability Ratio (LPR).","code":""},{"path":[]},{"path":"https://psychbruce.github.io/FMAT/reference/summary.fmat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"[S3 method] Summarize the results for the FMAT. ‚Äî summary.fmat","text":"","code":"# see examples in `FMAT_run`"},{"path":"https://psychbruce.github.io/FMAT/news/index.html","id":"fmat-20254","dir":"Changelog","previous_headings":"","what":"FMAT 2025.4","title":"FMAT 2025.4","text":"Added fill_mask() fill_mask_check(): functions technical check (.e., checking raw results fill-mask pipeline). Normal users usually use FMAT_run(). prefix.u2581: adding prefix ‚Å†\\u2581‚Å† mask words prefix.u0120: adding prefix ‚Å†\\u0120‚Å† non-starting mask words Now model information read model objects BERT_info() model initial commit date scraped HuggingFace BERT_info_date() saved subfolders local cache: /.info/ /.date/, respectively. Deprecated FMAT_load(). Fixed ‚ÄúR Session Aborted‚Äù issue MacOS (see #1). Sys.setenv(\"HF_HUB_DISABLE_SYMLINKS_WARNING\" = \"1\") Sys.setenv(\"TF_ENABLE_ONEDNN_OPTS\" = \"0\") Sys.setenv(\"KMP_DUPLICATE_LIB_OK\" = \"TRUE\") Sys.setenv(\"OMP_NUM_THREADS\" = \"1\")","code":""},{"path":"https://psychbruce.github.io/FMAT/news/index.html","id":"fmat-20253","dir":"Changelog","previous_headings":"","what":"FMAT 2025.3","title":"FMAT 2025.3","text":"CRAN release: 2025-03-19 Keep mind: function takes effect current R session temporarily, run time use FMAT functions R session. Added BERT_info_date(): Scrape initial commit date BERT models HuggingFace. Improved BERT_download() BERT_info(). Updated formal citation format JPSP article.","code":""},{"path":"https://psychbruce.github.io/FMAT/news/index.html","id":"fmat-20247","dir":"Changelog","previous_headings":"","what":"FMAT 2024.7","title":"FMAT 2024.7","text":"CRAN release: 2024-07-29 Added DOI link online published JPSP article: https://doi.org/10.1037/pspa0000396.","code":""},{"path":"https://psychbruce.github.io/FMAT/news/index.html","id":"fmat-20246","dir":"Changelog","previous_headings":"","what":"FMAT 2024.6","title":"FMAT 2024.6","text":"CRAN release: 2024-06-12 Fixed bugs: Now BERT_download() connects Internet, functions run offline way. Improved installation guidance Python packages.","code":""},{"path":"https://psychbruce.github.io/FMAT/news/index.html","id":"fmat-20245","dir":"Changelog","previous_headings":"","what":"FMAT 2024.5","title":"FMAT 2024.5","text":"CRAN release: 2024-05-19 Added BERT_info(). Added add.tokens add.method arguments BERT_vocab() FMAT_run(): experimental functionality add new tokens (e.g., --vocabulary words, compound words, even phrases) [MASK] options. Validation still needed novel practice (one ongoing projects), currently please use risk, waiting publication validation work. functions except BERT_download() now import local model files , without automatically downloading models. Users must first use BERT_download() download models. Deprecating FMAT_load(): Better use FMAT_run() directly.","code":""},{"path":"https://psychbruce.github.io/FMAT/news/index.html","id":"fmat-20244","dir":"Changelog","previous_headings":"","what":"FMAT 2024.4","title":"FMAT 2024.4","text":"CRAN release: 2024-04-29 Added BERT_vocab() ICC_models(). Improved summary.fmat(), FMAT_query(), FMAT_run() (significantly faster now can simultaneously estimate [MASK] options unique query sentence, running time depending number unique queries number [MASK] options). use reticulate package version ‚â• 1.36.1, FMAT updated ‚â• 2024.4. Otherwise, --vocabulary [MASK] words may identified marked. Now FMAT_run() directly uses model vocabulary token ID match [MASK] words. check [MASK] word model vocabulary, please use BERT_vocab().","code":""},{"path":"https://psychbruce.github.io/FMAT/news/index.html","id":"fmat-20243","dir":"Changelog","previous_headings":"","what":"FMAT 2024.3","title":"FMAT 2024.3","text":"CRAN release: 2024-03-22 FMAT methodology paper accepted (March 14, 2024) publication Journal Personality Social Psychology: Attitudes Social Cognition (DOI: 10.1037/pspa0000396)! Added BERT_download() (downloading models local cache folder ‚Äú%USERPROFILE%/.cache/huggingface‚Äù) differentiate FMAT_load() (loading saved models local cache). indeed FMAT_load() can also download models silently downloaded. Added gpu argument (see Guidance GPU Acceleration) FMAT_run() allow specifying NVIDIA GPU device fill-mask pipeline allocated. GPU roughly performs 3x faster CPU fill-mask pipeline. default, FMAT_run() automatically detect use available GPU installed CUDA-supported Python torch package (, use CPU). Added running speed information (queries/min) FMAT_run(). Added device information BERT_download(), FMAT_load(), FMAT_run(). Deprecated parallel FMAT_run(): FMAT_run(model.names, data, gpu=TRUE) fastest. progress bar displayed default progress FMAT_run().","code":""},{"path":"https://psychbruce.github.io/FMAT/news/index.html","id":"fmat-20238","dir":"Changelog","previous_headings":"","what":"FMAT 2023.8","title":"FMAT 2023.8","text":"CRAN release: 2023-08-11 CRAN package publication. Fixed bugs improved functions. Provided examples. Now use ‚ÄúYYYY.M‚Äù package version number.","code":""},{"path":"https://psychbruce.github.io/FMAT/news/index.html","id":"fmat-009-may-2023","dir":"Changelog","previous_headings":"","what":"FMAT 0.0.9 (May 2023)","title":"FMAT 0.0.9 (May 2023)","text":"Initial public release GitHub.","code":""},{"path":"https://psychbruce.github.io/FMAT/news/index.html","id":"fmat-001-jan-2023","dir":"Changelog","previous_headings":"","what":"FMAT 0.0.1 (Jan 2023)","title":"FMAT 0.0.1 (Jan 2023)","text":"Designed basic functions.","code":""}]
